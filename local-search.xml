<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>python相对模块引入与相对文件路径</title>
    <link href="/2024/10/23/python%E7%9B%B8%E5%AF%B9%E6%A8%A1%E5%9D%97%E5%BC%95%E5%85%A5%E4%B8%8E%E7%9B%B8%E5%AF%B9%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84/"/>
    <url>/2024/10/23/python%E7%9B%B8%E5%AF%B9%E6%A8%A1%E5%9D%97%E5%BC%95%E5%85%A5%E4%B8%8E%E7%9B%B8%E5%AF%B9%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84/</url>
    
    <content type="html"><![CDATA[<h2 id="x00-相对路径相对的是哪个路径">0x00 相对路径相对的是哪个路径</h2><p>首先我们要知道python会参考两个路径分别是<code>cwd</code>和<code>path</code>,其分别可以通过</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>cwd = os.getcwd() <span class="hljs-comment"># cwd 工作路径</span><br><br><span class="hljs-keyword">import</span> sys<br>path = sys.path <span class="hljs-comment"># path 环境变量</span><br></code></pre></td></tr></table></figure><p>我们发现在导入模块和读取文件时相对的是不同路径，这在读取和导入不再同一父目录的文件和模块时尤为重要。</p><p>我们有一个project其目录结构如图：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">|-- project</span><br><span class="hljs-string">|-- task</span><br>    <span class="hljs-string">|--test1.py</span><br>    <span class="hljs-string">|--test2.py</span><br>    <span class="hljs-string">|--11.txt</span><br><span class="hljs-string">|-- test1.py</span><br><span class="hljs-string">|-- test2.py</span><br><span class="hljs-string">|-- 11.txt</span><br></code></pre></td></tr></table></figure><h2 id="x01-cwd-工作目录">0x01 cwd 工作目录</h2><p>工作目录是python解释器运行的目录，程序去读取文件会更具cwd来寻找相对地址，<strong>且只会根据cwd，而与path无关</strong><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">在project目录下运行py文件</span><br>PS D:\tmp\project&gt; python task/test1.py<br>this is project/task/test1.py, now cwd is D:\tmp\project and sys.path[0] is D:\tmp\project\task<br><span class="hljs-meta prompt_"># </span><span class="language-bash">在task目录下运行py文件</span><br>PS D:\tmp\project\task&gt; python test1.py<br>this is project/task/test1.py, now cwd is D:\tmp\project\task and sys.path[0] is D:\tmp\project\task<br></code></pre></td></tr></table></figure>可以看到在不同的目录下使用python解释器运行文件会有不同cwd，而具有相同的syspath</p><h2 id="x02-sys-path">0x02 sys path</h2><p>syspath第一项是python文件所在的目录，在导入模块时，会去搜索path中的地址和相对地址，<strong>且与cwd无关</strong><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">运行project目录下的<span class="hljs-built_in">test</span></span><br>D:\tmp\project&gt; python test1.py<br>this is project/test1.py, now cwd is D:\tmp\project<br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行project/task文件下的<span class="hljs-built_in">test</span></span><br>PS D:\tmp\project&gt; python task/test1.py<br>this is project/test1.py, now cwd is D:\tmp\project and sys.path[0] is D:\tmp\project\task<br></code></pre></td></tr></table></figure> 可以看到sys path的第一项根据python文件进行更改。</p><h2 id="x03-根据相对地址导入模块">0x03 根据相对地址导入模块</h2><p>导入模块时相对的时sys.path中的地址。 ###在task/test1.py导入test2.py</p><p>在跑一个项目时，看到他在导入同一目录下文件时使用的是以下代码,并且该写法vscode的静态语法检查不会报错。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#file： project/task/test1.py</span><br><span class="hljs-keyword">import</span> task.test2.py<br><br></code></pre></td></tr></table></figure> 运行结果是 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">PS D:\tmp\project&gt; &amp; D:/ANACONDA/envs/GA/python.exe d:/tmp/project/task/test1.py<br>this is project/task/test1.py, now cwd is D:\tmp\project and sys.path[0] is d:\tmp\project\task<br>Traceback (most recent call last):<br>  File &quot;d:\tmp\project\task\test1.py&quot;, line 4, in &lt;module&gt;<br>    import task.test2<br>ModuleNotFoundError: No module named &#x27;task&#x27;<br></code></pre></td></tr></table></figure></p><p>通过sys path第一项我们可以看到其在d:</p><h3 id="解决办法">解决办法</h3><p>我们需要将父目录添加到环境变量中才能导入父目录这个文件夹。 * 方法1：将导入修改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> test2.py <br></code></pre></td></tr></table></figure> * 方法2： 我们要使我们的环境兼容他的代码 *在debug使可以在launch中添加父目录 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs json">    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.2.0&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;configurations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Python 调试程序: 当前文件&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;debugpy&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;request&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;launch&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;program&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;file&#125;&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;console&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;integratedTerminal&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;env&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;PYTHONPATH&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-comment">// 工作目录或者&quot;..&quot;</span><br>        <span class="hljs-attr">&quot;envFile&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;/.env&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br></code></pre></td></tr></table></figure> *也可以手动在powershell中为python添加环境变量 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ..<br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">env</span>:PYTHONENV=<span class="hljs-variable">$pwd</span></span><br></code></pre></td></tr></table></figure></p><h2 id="x04-根据相对地址导入文件">0x04 根据相对地址导入文件</h2><p>我在网上查找了很多资料，都是人都说相对路径相对的是当前文件的路径，其实不然。从<code>0x01</code>我们知道<code>cwd</code>会随着python调用的改变而改变，即使文件地址不变，在不同目录调用使用python文件会读到不同的文件</p><h3id="实验表明读取文件的相对地址是相对于cwd">实验表明读取文件的相对地址是相对于cwd</h3><p>test1.py代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;this is project/task/test1.py, now cwd is <span class="hljs-subst">&#123;os.getcwd()&#125;</span> and sys.path[0] is <span class="hljs-subst">&#123;sys.path[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;11.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-built_in">print</span>(f.read())<br></code></pre></td></tr></table></figure> 在不同目录调用test1的实验 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">在project目录中调用project/task/test1</span><br>PS D:\tmp\project&gt; &amp; D:/ANACONDA/envs/GA/python.exe d:/tmp/project/task/test1.py                                                                    <br>this is project/task/test1.py, now cwd is D:\tmp\project and sys.path[0] is d:\tmp\project\task<br>i am project/11.txt<br><span class="hljs-meta prompt_">#</span><span class="language-bash">在task目录中调用project/task/test1.py</span><br>this is project/task/test1.py, now cwd is D:\tmp\project\task and sys.path[0] is D:\tmp\project\task<br>i am project/task/11.txt<br>&#x27;&#x27;&#x27;<br><br>python 文件修改为open(&quot;../11.txt&quot;, &quot;r&quot;)，<br>&#x27;&#x27;&#x27;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">在task目录中调用project/task/test1.py</span><br>PS D:\tmp\project\task&gt; &amp; D:/ANACONDA/envs/GA/python.exe d:/tmp/project/task/test1.py<br>this is project/task/test1.py, now cwd is D:\tmp\project\task and sys.path[0] is d:\tmp\project\task<br>i am project/11.txt<br><span class="hljs-meta prompt_">#</span><span class="language-bash">在project目录中调用</span><br>this is project/task/test1.py, now cwd is D:\tmp\project and sys.path[0] is D:\tmp\project\task<br>Traceback (most recent call last):<br>  File &quot;D:\tmp\project\task\test1.py&quot;, line 4, in &lt;module&gt;<br>    with open(&quot;../11.txt&quot;, &quot;r&quot;) as f:<br>FileNotFoundError: [Errno 2] No such file or directory: &#x27;../11.txt&#x27;<br>&#x27;&#x27;&#x27;<br>文件会报错，因为在project的父目录，也即D:\tmp中没有11.txt文件。<br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure>可以看到path没有变，而cwd变化了，并且读取到了不同的txt文件。所以读取文件的相对路径依据的是cwd</p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>内核雏形</title>
    <link href="/2024/10/21/%E5%86%85%E6%A0%B8%E9%9B%8F%E5%BD%A2/"/>
    <url>/2024/10/21/%E5%86%85%E6%A0%B8%E9%9B%8F%E5%BD%A2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>-OS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kernel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>face impinting based on the diffusion model</title>
    <link href="/2024/10/16/face-impinting-based-on-the-diffusion-model/"/>
    <url>/2024/10/16/face-impinting-based-on-the-diffusion-model/</url>
    
    <content type="html"><![CDATA[<h2id="masked-face-inpainting-using-denoising-diffusion-probabilistic-models----ieee1-2024">MaskedFace Inpainting Using Denoising Diffusion Probabilistic Models---IEEE<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://ieeexplore.ieee.org/abstract/document/10691588">[1]</span></a></sup>2024</h2><p>这篇文章的实验比较水，但提供了一个方法。利用DDPM人脸补全方法，该文的方法是基于一个未修改的预训练DDPM.<br />原始的DDPM是从随机的高斯图像开始生成的，这与要修复的图像之间并无联系，<br />所以需要修改为图生图的方式。</p><h4 id="approach">approach</h4><p>修改训练过程，将需要补全图像作为一个条件输入，约束推理过程。<br />使用m来表示掩码，m可能是一个和图像一样大小的矩阵，其值为1的地方表示未被遮挡，值未0的地方表示被遮挡。所以我们可以使用m<em>x来生成未被遮挡区域图像， x </em> (1 - m)表示被遮挡区域。<br />我们需要经m*x作为条件输入到模型中。<br /><span class="math display">\[x^known = m\dot x x^unknwon = (1-m) \dot x\]</span></p><p><span class="math inline">\(x^knwon\)</span> 是推理开始已知的 <spanclass="math inline">\(x^unknwon\)</span>是要推理出来补全在<spanclass="math inline">\(x^knwon\)</span>中的。<br /> 这是推理过程中的公式。从<spanclass="math inline">\(x_t\)</span>推理到<spanclass="math inline">\(x_0\)</span>.<br />每一个<span class="math inline">\(x_t\)</span>是由<spanclass="math inline">\(x^knwon_{t-1}\)</span> 和<spanclass="math inline">\(x^unknwon_{t-1}\)</span>组合起来的。 其中<spanclass="math inline">\(x^knwon\)</span>是遮挡图像直接加噪来的，不需要去噪。<spanclass="math inline">\(x_t^unkown\)</span>应该是从高斯噪声去噪来的。<br />从公式的意义上来理解，可以认为让denoise参考未遮挡区域，但不对未遮挡区域进行修改,不修改体现为最后用<spanclass="math inline">\(x^known\)</span>进行覆盖，保留对遮挡区域去噪的结果，得到的<spanclass="math inline">\(x_{t-1}\)</span>由可以在下一次denoise中被参考。</p><p>该文章除了训练集以外和人脸图像并无关联，且也没有验证补全图像对人脸识别的影响，还有很多的改进空间。</p><h2id="repaint-inpainting-using-denoising-diffusion-probabilistic-models2---cvpr2022">RePaint:Inpainting using Denoising Diffusion ProbabilisticModels<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html">[2]</span></a></sup>--cvpr2022</h2><p>这篇文章的修复方法和上一篇是一样的，考虑到发布时间，上一篇应该是基于这一篇写出来的。两篇文章的算法介绍一模一样。<br />采用与预训练DDPM模型开源<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://github.com/andreas128/RePaint`">[3]</span></a></sup>。才用diffusion的缺点是推理的速度很慢。<br />除了diffusion方法外，大多数图像生成基于GAN。<br />模型仅利用现成的DDPM模型，模型不争对修复任务进行训练。而是在推理过程中利用给定像素来调节。<br /></p><p>这里再说一下推理算法，由于再推理过程中需要对掩码区域编码得到m。在实际的应用中，被口罩墨镜等物品遮挡的区域需要先被识别出来再编码得到m，这可能需要用到一些人脸遮蔽检测模型来"画出"被遮挡区域。</p><p>结合这篇文章的补全方法和arcface，领域适应迁移学习，也许能达到很好的效果。我们可以通过补全模型生成多个可能的补全图像，比如一个口罩遮蔽人脸，其鼻子嘴唇形状上是未知的，生成多种组合的补全图像，能产生多个相似度对比参数，可以设计一种取舍策略来判断是原始图像是否匹配。diffusion的生成图像的特点即为从一个随机高斯噪声开始，所以每一次补全都能生成不同的图像。</p><h3 id="重采样方法">重采样方法</h3><p>如果只通过一遍采样，补全的图像缺少整体的语义信息，即补全的图像与相邻区域材质相同，而无法产生形状。从knwon图像加噪来的图像没有考虑生成的图像，这带来的不和谐。模型需要毒刺通过重采用减慢了图像反向扩散的速度，让图像再每一步扩散中多学习几次<br />从knwon图像加噪</p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span>https://ieeexplore.ieee.org/abstract/document/10691588<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2"class="footnote-text"><span>https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3"class="footnote-text"><span>https://github.com/andreas128/RePaint`<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-face impinting</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PE文件-最小化</title>
    <link href="/2024/10/15/PE%E6%96%87%E4%BB%B6-%E6%9C%80%E5%B0%8F%E5%8C%96/"/>
    <url>/2024/10/15/PE%E6%96%87%E4%BB%B6-%E6%9C%80%E5%B0%8F%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2id="对tinype1的简单总结">对TinyPE<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="http://www.phreedom.org/research/tinype/">[1]</span></a></sup>的简单总结</h2><ul><li>在windows2000上的最小PE文件为： 133bytes</li></ul><h3 id="x00-压缩mz文件头">0x00 压缩MZ文件头</h3><p>DOS文件头中只有两个区域需要用，e_magic("MZ")和e_lfanew(PE头偏移)，且MS-sub是不需要的，可以整个删除。先将MZ头的剩余位填充为0。但是MZ头是不能删除的。MZ头的大小是0x3c<br />通过修改e_lfanew的值小于0x3c使pe头从MZ头的内部开始。<br />由于需要MZ头和PE头对齐，所以最小的PE头开始位置是0x04<br />e_lfanew和sectalign位于相同的偏移，所以值都为0x04</p><h3 id="x01-压缩pe文件头">0x01 压缩PE文件头</h3><p>修改PE文件头中的文件对齐字段为1，这样节不需要从0x200开始。<br />通过将NumberOfRvaAndSizes设置为0可以将数据目录从可选文件头删除。</p><h3 id="x02-压缩pe节表头">0x02 压缩PE节表头</h3><p>节表开始的计算方式是可选文件头开始加上可选文件头大小。<br />通过将可选文件头大小字段设置的小于实际大小，让节表头重叠在可选文件头中。</p><h3 id="x03-压缩代码节">0x03 压缩代码节</h3><p>将code代码放在未使用的PE头字段中，如时间戳。</p><h3 id="x04-压缩引入文件节">0x04 压缩引入文件节</h3>删除导入查找表，是IAT的副本。然后还可以折叠IAT和DLL名称，将其分别放在节表名称和PE可选头的末尾的未使用字节处## 参考<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span>http://www.phreedom.org/research/tinype/<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>软件安全</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PE,</tag>
      
      <tag>pwn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>操作系统Loader加载</title>
    <link href="/2024/10/14/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FLoader%E5%8A%A0%E8%BD%BD/"/>
    <url>/2024/10/14/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FLoader%E5%8A%A0%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<p>由于引导扇区的大小被局限为了512字节，这对于我们的是远远不够的，所以我们通过再建立一个文件，通过引导扇区把他加载到内存，再把控制权交给他，这样就没有512字节的限制了。引导扇区的代码只负责把Loader加载到内存中，再由Loader来加载内核进入保护模式。我们创建一个文件系统来管理loader和内核代码。我们下面的目的是把Loader放到文件系统中，以让引导扇区找到并加载他。</p><h2 id="fata12">FATA12</h2><p>FATA12的结构依次是，引导扇区，FAT表1，2 ， 根目录， 数据区。<br />引导扇区：是一个数据结构，BPB_,BS_开头的属性，例如BS_jmpBoot,共512字节，BPB_BytePerSec每扇区字节数，还有加载Loader的程序由jmp跳转FAT表：有两个，每个9个扇区，存储的文件在数据区中的簇号链表，FAT项的值表示下一个簇号FFF结束。根目录：存储的是文件的基本信息，如文件名，文件属性，文件开始簇号</p><h2 id="加载loader">加载Loader</h2><p>加载Loader需要读取软盘，运用int 13h中断来读取。将其读到es:bx中。</p>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>中断与异常</title>
    <link href="/2024/10/12/%E4%B8%AD%E6%96%AD%E4%B8%8E%E5%BC%82%E5%B8%B8/"/>
    <url>/2024/10/12/%E4%B8%AD%E6%96%AD%E4%B8%8E%E5%BC%82%E5%B8%B8/</url>
    
    <content type="html"><![CDATA[<h2 id="introduce">introduce</h2><p>在实模式下直接使用BIOS中断，而在保护模式下不能使用，需要IDT表来代替。（InterruptDescriptor Table）<br />在IDT表中使用中断门和陷阱们，Gate的结构由选择子，属性和偏移组成，利用选择子和偏移来找到中断程序入口</p><h2 id="中断和异常机制">中断和异常机制</h2><p><strong>中断</strong>：程序程序执行时因为硬件而随机发生，通常用来处理外部时间，如外围设备请求，int n也可以产生中断。<br /><strong>异常</strong>: 处理器在执行时遇到错误，如零除，页错误等。 ###异常的三种类型 * Fault是一种可以被更正的异常，一旦被更正，程序可以继续运行。异常处理程序处理完后，返回执行产生fault的指令。* Trap也是一种允许程序继续执行的异常，但是返回地址是产生Trap之后的那条指令。 *Abort 发生异常后不允许程序继续执行，是用来报告严重错误的。</p><h3 id="外部中断和内部中断">外部中断和内部中断</h3><ul><li>内部中断 由int n 产生，类似于调用门的使用。</li><li>外部中断由硬件产生的中断，外部中断分为不可屏蔽中断NMI和可屏蔽中断INTR。不可屏蔽中断对应向量号2，可屏蔽中断于cpu是通过8259A连接的。8259A是用来对所有外部设备的一个代理，可以优先级处理中断和屏蔽打开中断。外部中断需要建立硬件和中断向量的联系。</li></ul><h4 id="a芯片">8259A芯片</h4><p>通过设置8259A将中断请求与中断向量对应起来。中断处理由两片8259A芯片级联，一共可以挂载15个不同的外部设置<br />芯片的设置必须按顺序向主从芯片中写入ICW1、ICW2、ICW3、ICW4，其分别设置 *ICW1 ： 单个还是级联，需不需要ICW4，中断向量字节数 * ICW2 ：芯片中断向量的起始值，如给主8259A发送020h，表示IRQ0-IRQ7对应向量号020h-027h* ICW3：主从芯片的连接端口，如给主芯片发送004h，代表主芯片的IR2对应从芯片。给从芯片发送002h代表连接主片的IR2</p><p>OCW用于屏蔽或打开中断，OCW1的格式很简单，对应位为0代表打开1代表关闭。OCW2用来发送EOI。EOI是中断处理结束后需要发送给8259A,以便继续接受中断的。</p><h3 id="建立idt">建立IDT</h3><ol type="1"><li>初始化时直接创建255个描述符即可。</li><li>将想加入的中断按找IDT表的偏移加入到IDT表中Gate的结构由选择子，属性和偏移组成，利用选择子和偏移来找到中断程序入口例如：在80h偏移处加入中断 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs asm">.80h:   Gate  SelectorCode32, UserINtHandler, 0, DA_386IGate<br><br></code></pre></td></tr></table></figure>补充UserIntHander程序后，就可以通过int 80h进行调用。</li></ol>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>diffusion model</title>
    <link href="/2024/10/10/diffusion-model/"/>
    <url>/2024/10/10/diffusion-model/</url>
    
    <content type="html"><![CDATA[<h2 id="马尔可夫链">马尔可夫链</h2><p>stable Diffusion的前向过程和反向过程都是一个马尔可夫连，<br />马尔科夫链的思想：过去所有的信息都已经被保存到了现在的状态，基于现在就可以预测未来。马尔科夫链认为过去所有的信息都被保存在了现在的状态下了。比如这样一串数列 1 - 2 - 3 - 4 - 5 - 6，在马尔科夫链看来，6 的状态只与5 有关，与前面的其它过程无关。<br />既然某一时刻状态转移的概率只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转换概率，这个马尔科夫链的模型就定了</p><h3 id="马尔可夫矩阵的稳定性">马尔可夫矩阵的稳定性</h3><p>状态转移矩阵有一个非常重要的特性，经过一定有限次数序列的转换，最终一定可以得到一个稳定的概率分布，且与初始状态概率分布无关</p><h2 id="diffusion-models">DIffusion Models</h2><p>DDPM（Denoising Diffusion ProbabilisticModel）于2020年被提出，被称为扩散模型，是如今一种先进的图像生成模型。</p><h3 id="模型预测过程">模型预测过程</h3><p>DDPM模型从随机噪声开始，通过多步预测，<spanclass="math inline">\(X_i\)</span>都会在<spanclass="math inline">\(X_{i-1}\)</span>的基础上减少一些噪声，让图片更清晰。模型在每一步学习的不是从<spanclass="math inline">\(X_{i-1}\)</span>到<spanclass="math inline">\(X_i\)</span>的函数，而是从<spanclass="math inline">\(X_j{i-1}\)</span>到<spanclass="math inline">\(X_i\)</span>的差值（噪声），类似于resnet中的残差，从<spanclass="math inline">\(X_{i-1}\)</span>减去噪声得到<spanclass="math inline">\(X_i\)</span>。同时在每一步时还会加上一个步数编码。</p><h3 id="模型训练过程">模型训练过程</h3><p>从预测过程可知，模型要学习的groundtruth是相对于上一张图的噪声，所以训练样本也应该是噪声。为了得到这个训练噪声，我们可以通过ForwardProcess（Diffusion Process）来获得噪声。<em>forwardprocess</em>即对每一步的图像随机采样噪声，得到<code>噪声</code>和<code>加上噪声</code>的<spanclass="math inline">\(X_{i+1}\)</span>,所以grount truth就是在<em>forwardprocess</em>中得到的<code>噪声</code></p><h2 id="文生图framework">文生图Framework</h2><p>文生图的三个网络： 1. Text Encoder： 从输入的文字生成词向量 2.Generation Model ； 结合初始随机噪声和词向量representationfeture或者小图 3. Decoder 从representation feture 生成最终图像一般来说三个网络是分开训练的。在有的模型中<code>2</code>和<code>3</code>可以是一个模型。</p><h3 id="text-encoder-的作用">text encoder 的作用</h3><p>帮助图像更好的理解，提高模型的泛化能力。逻辑上，模型在训练集上见到的文字描述是有限的，如果遇到没见的描述可能会使模型理解能力变差，所以通过encoder可以将模型不能理解文字转化为可以理解的向量。</p><h3 id="指标">指标</h3><h4 id="fid">FID</h4><p>有一个训练好的分类模型。例如vgg图像识别模型。将真实图像和生成图像在vgg中产生的represetation算一个distance。类似GAN中的discriminator。</p><h4 id="clip-contrastive-language-image-pre-training">CLIP ContrastiveLanguage-image Pre-training</h4><p>这是一个模型，可以对text和image算distance。<br />如果这个text使在描述这个image则distance越低。</p><h3 id="decoder">Decoder</h3><ol type="1"><li>如果generatation产生的是一些小图，则直接通过图像处理产生大图小图训练集，扔给模型。decoder是一个图像超分模型。</li><li>如果generatation产生的是一些latentRepresentation。可以利用一个Autu-encoder自编码。 &gt;Q:这个decoder如何跟latent Representation对齐呢？<br />A： 1.先训练decoder，再锁住decoder再去训练generation。2.最后来一个端到端训练。<br />Q:对于1如何再训练generation使使它对齐呢。</li></ol><h3 id="vae-variational-auto-encoder-变分自编码器">VAE variational autoencoder 变分自编码器</h3><h3 id="算法-training">算法 training</h3><p><span class="math inline">\(X_0\)</span>：clear image<br />Uniform : 均匀分布<br /><span class="math inline">\(\epsilon\)</span> ：noise<br /><span class="math inline">\(\epsilon_\theta\)</span>: Noisepredictor<br /><code>2</code>： sample one clear image<br /><code>3</code>: sample a number between 1, T<br /><code>4</code>: sample a noise from Normal distribution<br /><code>5</code>: 圆括号内对<spanclass="math inline">\(x_0\)</span>和<spanclass="math inline">\(\epsilon\)</span>做wighted sum，其中<spanclass="math inline">\(\bar{\alpha_t}\)</span>是一组事先选择的权重，从大到小。t是<code>2</code>中随机选择的。sample到的t越大,<spanclass="math inline">\(\alpha\)</span>越小，原图占的比例越小。加起来就是noiseimage.noise image 和 t一起作为predictor的输入</p><blockquote><p>想象中reverse process过程是利用predictor一步步减少噪音，即每一步把噪声很多的图减少到噪声不那么多的图，但是再实际算法中DDPN直接在clearimage中混入噪音，只不过噪音的大小由t来决定，然后学习噪音。没有中间少量噪音的中间图。这是经过数学推导得到的。</p></blockquote><h3 id="算法-sampling">算法 Sampling</h3><p><code>1</code>: sample a noise image from normal distribution<spanclass="math inline">\(X_T\)</span><br /><code>2</code>: loop T times<br /><code>3</code>: sample a noise <spanclass="math inline">\(z\)</span><br /><code>4</code>: 括号里的是<span class="math inline">\(X_t -predict_noise\)</span>，系数是公式推导出的，<spanclass="math inline">\(/alpha\)</span>和training算法中类似。加z是提升泛化，不加似乎出不来。</p><h3 id="公式推导">公式推导</h3><p>看了一遍没太看懂，过段时间再看吧<br />看懂了，但不想写了，扔个链接 <ahref="https://zhuanlan.zhihu.com/p/650394311">深入浅出扩散模型(DiffusionModel)系列：基石DDPM（人人都能看懂的数学原理篇）</a></p><h3 id="一个mnist的demo">一个mnist的demo</h3><p>https://github.com/guchengzhong/latent_diffusion_model_mnist.git</p><h3 id="unet-框架">Unet 框架</h3><h3id="guided-diffusion3">guided-diffusion<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://arxiv.org/pdf/2105.05233#page=16&amp;zoom=100,144,174">[3]</span></a></sup></h3><p>一种使用了classifier的diffusion,openai的仓库地址<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://github.com/openai/guided-diffusion">[2]</span></a></sup></p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span>https://zhuanlan.zhihu.com/p/655568910<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2"class="footnote-text"><span>https://github.com/openai/guided-diffusion<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3"class="footnote-text"><span>https://arxiv.org/pdf/2105.05233#page=16&amp;zoom=100,144,174<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>diffusion</tag>
      
      <tag>cv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex中文字体</title>
    <link href="/2024/10/01/Latex%E4%B8%AD%E6%96%87%E5%AD%97%E4%BD%93/"/>
    <url>/2024/10/01/Latex%E4%B8%AD%E6%96%87%E5%AD%97%E4%BD%93/</url>
    
    <content type="html"><![CDATA[<p>今天在使用老师给的latex模板时，一直因字体原因报错。网上查了才知道，不同系统下使用的字体时不同的，模板的的字体指定的时Windows下的字体，而我的texlive下载Ubuntu中，所以会缺少很多windows自带的字体，于是只能删掉linux中的tex，再windows中重新下一遍。可怜我windows中所剩不多的空间啊。/::&gt;_&lt;::/</p><p>例如出现错误 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Package fontspec Error: The font &quot;SimSun&quot; cannot be found<br></code></pre></td></tr></table></figure>在windows环境下就不会有这个问题，在linux下就需要自定义安装。</p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>字体</tag>
      
      <tag>bug</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>页式存储</title>
    <link href="/2024/09/30/%E9%A1%B5%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    <url>/2024/09/30/%E9%A1%B5%E5%BC%8F%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="地址转换流程">地址转换流程</h2><p>逻辑地址----分段机制---&gt;线性地址----分页机制---&gt;物理地址</p><p>实现分段机制，可以提供保护，而实现分页机制，可以实现虚拟内存，使内存管理变得灵活。</p><h2 id="页表">页表</h2><p>对于一个两级页表来说，页表分为页目录和页表cr3：寄存器，指向页目录开头PDE：页目录表的表项，指向某一个页表的开头<br />PTE：页表的表项，指向某一个物理页在启动页表前需要将页表内容初始化。<br />分页机制是否生效取决于cro的最高位PG位，所以开启页表只需要将cr3指向页目录表，设置PG位=1.</p><h2 id="启动页表">启动页表</h2><h3 id="初始化页表">1. 初始化页表</h3><p>在启动页表前需要将页表内容初始化。在页目录中填入页表的地址，页表中填入物理页的地址。在具体的初始化方法中，操作系统更具硬件内存的大小来设置页表的个数。使用int 15h获取内存信息 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asm">;需要设置的寄存器代表的意义<br>eax = 0xe820 ;工作号<br>ebx = 0x0 ;后续值<br>es:di ;指向地址范围描述符结构<br>ecx ;地址范围描述符的大小<br>edx = 0x534d4150 ;签名<br>;返回结果<br>CF ; 返回0代表没有错误<br>eax ; 0x534d4150<br>es:di ; 地址范围结构描述符，与输入相同<br>ecx ; 地址范围描述符的大小<br>ebx ; 下一个地址范围描述符需要的后续值<br></code></pre></td></tr></table></figure> 地址范围描述符 <imgsrc="页式存储\image.png" alt="alt text" /></p><h3 id="启动分页">2.启动分页</h3><p>分页机制是否生效取决于cro的最高位PG位，所以开启页表需要将cr3指向页目录表，设置PG位=1.不同的程序可以通过不同的cr3来拥有不同的页表，相同的地址。</p><h3 id="pte属性位">pte属性位</h3><ul><li>前20位：表示物理页的地址</li><li>P位：表示页是否在内存中，1表示在内存中，0表示不在内存中，需要从硬盘中读取。</li><li>R/W位：表示页是否可写，1表示可写，0表示只读</li><li>U/S位：表示页是否用户态可访问，1表示用户态可访问，0表示只有内核态可访问</li><li>A位：表示页是否被访问过，1表示被访问过，0表示没有被访问过</li><li>D位：表示页是否被修改过，1表示被修改过，0表示没有被修改过</li><li>G位：表示页是否被全局页表引用，1表示被全局页表引用，0表示没有被全局页表引用</li><li>PAT位：表示页是否支持高速缓存，1表示支持，0表示不支持</li><li>PS位：表示页是否是大页，1表示是大页，0表示不是大页</li><li>NX位：表示页是否可执行，1表示不可执行，0表示可执行</li></ul>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>网络层</title>
    <link href="/2024/09/30/%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
    <url>/2024/09/30/%E7%BD%91%E7%BB%9C%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="网络层">网络层</h1><p>也可以看作IP层，有ip，ARP等协议。网络层的主要功能是将数据从一个网络发送到另一个网络中。网络层可以分为数据层和控制层。网络层的数据传输是不可靠的，即是会丢失的，数据传输的可靠性是靠上层协议保障的，如tcp协议。## 1. 数据层 ### 1.1 IP协议<code>网络协议IP协议</code>，<code>地址解析协议ARP</code>,<code>网际控制报文协议ICMP</code>,<code>网际组管理协议IGMP</code>。 #### 1.1.1 分类ip分为网络号和主机号，网络号用来标识网络，主机号用来标识主机。ip地址分为A,B,C,D,E五类，A,B,C三类是常用的，D,E两类是保留的。A类地址是8位主机号，B类16位，C类24位。A类网络号的0和127被保留作为本网络和本地软件测试。其余类不管对于所有主机号，都会保留全零作为本主机和全1作为所有主机。 #### 1.1.2无分类ip CIDRCIDR是无分类的ip地址，CIDR是为了解决ip地址的浪费问题，CIDR是将ip地址分为网络前缀和主机号，网络号和主机号的分界线是通过子网掩码来确定的。子网掩码是32位的二进制数，前面是网络号，后面是主机号。子网掩码的1表示网络号，0表示主机号。子网掩码的1的个数是网络号的位数。子网掩码也可以表示网络前缀的位数，如20<em>ip的分配不是按单个地址来的，而是先分配一个连续的地址块，在对每个主机分配这个地址块中的主机号ip</em><strong>CIDR地址块</strong>：把网络前缀相同的所有连续ip称为一个地址块，</p><p><strong>路由聚合</strong>：在路由器的转发表中利用一个较大的CIDR地址块来替代许多小的地址块。</p><h4 id="ip的特点">1.1.3 ip的特点</h4><ul><li>ip由网络前缀和主机号组成，ip地址是分等级的，ip管理机构**只分配网络前缀，主机号由得到网络前缀的单位自行分配。路由器更具网络前缀来转发，可以减少路由表大小。</li><li>ip地址标志一个接口，如果一个主机在两个网络上，那么他有两个ip，其网络前缀不同。由于一个路由器一定连接两个网络，所有其至少有2个ip。</li><li>在互联网上，具有相同的网络前缀的主机在一个网络上，通过以太网交换机连接起来的网络仍是一个网络。不同前缀的必须由路由器连接。</li><li>以太网交换机是没有ip地址的，路由器和以太网交换机每个接口都有一个mac地址。</li></ul><h3 id="地址解析协议arp">1.2 地址解析协议ARP</h3><p>解决如何在已知要发往的IP地址时找到其相应的MAC地址。ARP的方法是在主机的ARP高速缓存中存放一个IP地址到MAC地址的映射表，并动态更新。#### 1.2.1 ARP协议步骤 1.在局域网中广播ARP请求分组，主要内容是，我的IP地址，我的MAC地址，请求的IP地址。2. 在局域网上的所有主机上运行的ARP进程都会受到ARP请求分组。 3.如果IP与ARP请求分组一致，就收下ARP请求，并发送ARP响应分组并填入自己的MAC地址<strong>ARP响应式单播的</strong>4. 主机受到ARP响应后将IP，MAC映射写入。 ### 1.3 IP数据报的格式以4个字节为一个单位，固定部分20字节  #### 1.3.1 ip分片是否对ip分片，取决于ip报的长度不能长于链路层MTU，如果过长就要分片。<br /><strong>标识</strong> ：表示几个分片式同一个数据包切片的<strong>标志</strong> ：表示该数据报可不可以分片 <strong>片偏移</strong>：表示分片在原分组的相对位置，单位为8个字节。 ### 1.4 IP分组转发若ip转发基于分组首部的目的地址，称为基于终点的转发。 ### 1.5 ICMP协议ICMP是ip的一个附属协议，主要用于网络故障诊断和错误报告。ICMP报文是封装在ip数据报中的，ICMP报文的首部是8字节，数据部分是变长的。ICMP协议有： 1. ICMP差错报告返回主机不可达，网络不可达，端口不可达等。如当报文的TTL为0时，路由器会返回一个ICMP差错报告。2. ICMP请求和应答 ping命令会用到这个类型来查询延迟。 3.ICMP重定向报告</p><h3 id="ipv6协议">1.7 ipv6协议</h3><p>主要的改进点是基本首部长度固定为40字节，不再分片，不再有校验和，增加了流标签字段，增加了扩展首部。分片放在扩展首部上。</p><h2 id="控制层">2 控制层</h2><h3 id="路由选择协议">2.1 路由选择协议</h3><p>路由选择协议是网络层的控制层，主要是用来选择路由的。路由选择协议有很多种，如静态路由。动态路由：RIP，OSPF（内部网关协议），BGP（外部网关协议）等。互联网被划分为多个自治系统AS，每个自治系统内部可以使用不同的路由选择协议，自治系统之间使用BGP协议。#### 2.1.1 内部网关协议 路由信息协议RIPRIP协议要维护的路由表中的信息是到每个网络的距离，以及应经过的下一跳地址。<br />在信息交换时，和相邻路由器交换路由表。RIP协议的一个特点是好消息传的快，坏消息传的慢。RIP协议的最大跳数是15，超过15就认为不可达。RIP协议的缺点是收敛速度慢，不适合大型网络。#### 2.1.2 内部网关协议 OSPFOSPF协议向网络中的路由器发送信息，确定其与相邻路由器的连接状态，得到网络全局的拓扑图，然后根据这些信息计算出最短路径。OSPF协议的特点是收敛速度快，适合大型网络。#### 2.1.3 外部网关协议 BGP在一个自治系统AS中有两种不同功能的路由器，即边界路由器和内部路由器。BCG路由中的信息包含:前缀，AS路径，下一跳地址，度量值。<br />BGP协议的特点是路由信息的传播是有序的，可以避免环路。BGP协议的缺点是收敛速度慢，适合大型网络。## 3 路由器 <strong>路由表</strong>：包含从目的网络到吓一跳（IP地址）的映射， <strong>转发表</strong>：包含从目的网络到下一跳（MAC地址）的映射。转发表是从路由表中生成的。<br />在实际讨论中可以将路由表和转发表合并为一个表，即路由表。 ###4.网际组管理协议 IGMP IGMP工作的连个阶段： 1.主机加入组播组，向多播地址发送IGMP报文。本地多播路由器接受到后把组关系转发给互联网上的多播路由器2.本地多播路由器周期性的探询本地局域网的主机，如果没有主机响应，则认为本网络上的主机已经离开这个组，停止转发组关系给其他多播路由器。<strong>多播路由器不需要知道组成员的信息，只需要知道本地网络上是否存在组成员，因为转发是硬件多播</strong><br /><strong>多播的发送者和接收者都不是到组成员有多少。</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>物理层</title>
    <link href="/2024/09/30/%E7%89%A9%E7%90%86%E5%B1%82/"/>
    <url>/2024/09/30/%E7%89%A9%E7%90%86%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="物理层">物理层</h1><p>物理层考虑的是如何在传输媒体上传输比特流。<br />硬件设施和传输媒体种类很多，可以传输所有传输媒体。<br /><strong>物理层的主要任务也可以描述为确定传输媒体的接口有关的一些特性 --如接口大小，排列，电器特性，电压。</strong> ## 1.通信基础知识 ## 1.1基本概念 <strong>通信模型</strong> 包括<br />源系统<br />------------ 1. 源点 -- 数据 2. 发送器 -- 对源点的数据进行编码------------ 传输系统： 传输线，复杂网络 ------------ 目的系统 3. 接收器-- 对传输系统的信号进行解码 4. 终点 --终点设备从接收器获取的数字比特流</p><p>通信系统包含计算机网络，电话系统，广播电视系统，无线通信系统等。</p><p><strong>信道</strong>表示向某一个方向发送信息的媒体，一条通信电路包含一条<em>发送信道</em>和一条<em>接受信道</em>.从通信的信息交互方式来看，通信可以分为三种： * 单行通信 （单工通信） --只有一个方向通信，没有反向交互 * 双向交替通信（半双工通信）--都可以发送和接收，但不能同时进行 * 双向同时通信（全双工通信）-- 可以同时发送和接收</p><p><strong>基带信号</strong>来自信源的信号被称为基带信号，计算机的文字数据信号属于基带信号，基带信号是一种非常低频的信号，不能直接传输，需要经过调制变成高频信号才能传输。</p><p><strong>调制</strong> 将基带信号于信道特性向适应。 * 基带调试调试后依然是基带信号。又称为编码，把数字信号转换为另一种数字信号 *带通调制使用载波调制，将数字信号转化为模拟信号，调制后的信号称为带通信号。</p><p><strong>编码</strong>  <strong>带通调制</strong> * 调幅（AM）：调制信号的幅度随基带信号的变化而变化 *调频（FM）：调制信号的频率随基带信号的变化而变化 *调相（PM）：调制信号的相位随基带信号的变化而变化</p><h2 id="信道极限容量">1.2 信道极限容量</h2><p><strong>信道能通过的频率</strong>高频信号会引起码间串扰，使每一个码元间失去界限。如，信道宽度为4000Hz则最高码元传输速率为每秒8000个。 -<strong>纳什准则</strong>在带宽为W的信道中，码元最高传输速率是2W，超过上限信号无法识别。<strong>信噪比</strong> - <strong>信噪比</strong>是信号功率和噪声功率的比值，是衡量信号质量的重要指标。表示为<spanclass="math inline">\(S/N\)</span>。 - <strong>香农公式</strong>信道极限容量 <span class="math display">\[C = W \log_2(1 + S/N)  (bit/s)\]</span> 其中<spanclass="math inline">\(W\)</span>是信道宽度（Hz）。<spanclass="math inline">\(S\)</span>是信号功率，<spanclass="math inline">\(N\)</span>是噪声功率。<br />香农公式指出信噪比越大，信道传输速率越高。给出了上限<br />纳什准则鼓励让码元由更高的信息量，香农公式告诉了信息传输的绝对上限</p><h2 id="传输媒体">1.3 传输媒体</h2><p><strong>传输媒体</strong>是信息传输的通道，是发送器和接收器之间的物理通路，可以分为导向传输媒体和非导向传输媒体。1. 导向型 -- 有线固体导引传播 2. 非导向型 -- 无线传播 ### 1.3.1 导向型<strong>双绞线</strong>由两根绝缘铜线组成，每根线中有许多细铜线，细铜线被绝缘包裹，两根线绕在一起。通信线路一般为几公里。距离太长会由衰减。 <strong>同轴电缆</strong>由一根铜线和一层绝缘层组成，铜线被绝缘包裹，再由一层金属网层包裹，最外层是绝缘层。<strong>光缆</strong>光纤由非常透明的石英玻璃拉成丝，由纤芯和包层组成。光波在纤芯中传播，由光脉冲为1，无光脉冲为0## 1.3.2 非导向型 <strong>无线传播</strong>无线传播是通过电磁波传播，电磁波是由电场和磁场交替变化而产生的波动。</p><h2 id="信道复用">1.4 信道复用</h2><p><strong>信道复用</strong>是指多个用户共享一个信道，提高信道的利用率。信道复用技术有频分复用，时分复用，波分复用，码分复用。1. 频分复用（FDM） 不同信道使用不同的带宽（频率） 2. 时分复用（TDM）在一个TDM帧内每一路占用固定时间段 ## 1.5 宽带接入技术用户要上网，需要连接到ISP，已得到IP地址。<br /><strong>互联网早期</strong> 用户采用电话线路连接到ISP，拨号上网<strong>宽带接入技术</strong> 1.ADSL可以复用吸纳有电话网中的用户线。借助用户线两端的ADSL调制解调器对数字信号进行调制。2.HFC（光纤同轴混合网路）主干网采用高宽带光纤传输，在光纤节点将光纤信号转换为电信号，通过同轴电缆传输到用户家中。由于同轴光缆是用户共享的，所以用户带宽在许多用户传输数据时会有波动。3.FTTH（光纤到户）用户家中直接接入光纤，带宽高，速度快，但是成本高。OLT是连接到光纤主干的光纤终端，OLT把下行数据发往无源的1：N光分路器，然后<em>广播</em>到用户家中的ONU。接受数据时ONU根据表示只接受发送给自己的数据。</p>]]></content>
    
    
    <categories>
      
      <category>计算机网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>数据链路层</title>
    <link href="/2024/09/30/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
    <url>/2024/09/30/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="数据链路层">数据链路层</h1><p>网络层是将包从一个网络通过路由发送到另一个网络中，数据链路层则是如何在同一个局域网中从一条主机发送到另一台主机，但不经过路由器转发。<br />数据链路层的网络分为： 1. 点对点 2. 广播</p><h2 id="透明传输">1 透明传输</h2><p>数据链路层会对数据加上帧开始符号和帧结束符，来对帧进行区分。为了解决数据中出现的帧开始符号和帧结束符号，数据链路层会对数据中出现的帧开始符号和帧结束符号进行转义。即字节填充方法，传输数据时对数据中的特殊字符添加转义符号，接收端接收到数据后将转义字符去掉。## 2.CRC校验能检测传输过去的帧是否由bit位错误，在当前讨论的数据链路层中，不要求可靠数据传输，可靠数据传输由上层协议保证。这是由于现在通讯线路质量好，不使用确认和重传可以提高传输效率</p><h2 id="点对点协议ppp">3. 点对点协议PPP</h2><p>数据链路层<strong>简单</strong>的思想：接收方每接受到一个帧，就进行CRC校验，如果校验正确，就收下这个帧，反之丢弃，其他什么都不做。</p><h3 id="ppp协议组成">3.1 PPP协议组成</h3><ol type="1"><li>PPP协议对IP数据报进行封装，信息最大长度位MTU</li><li>建立链路连接的<strong>链路控制协议LCP</strong></li><li>一套<strong>网络控制协议NCP</strong>，其中一个协议支持不同的网络层协议，如IP，IPX，AppleTalk等</li></ol><h4 id="帧格式">3.1.1 帧格式</h4><p>首部4个字段 * 标志字段：0x7E 帧开始或结束 * 地址字段：0xFF 无地址 *控制字段：0x03 无控制（最初是留给以后用的，但到现在也没用） *协议字段：如 0x0021 IP协议 0x0023 ARP协议 尾部2个字段 * CRC校验：FCS *帧结束符：0x7E</p><h4 id="字节填充">3.1.2 字节填充</h4><p>在异步传输时（逐字符发送）为了实现透明传输，采用字节填充方法， 1.当信息字段中出现0x7E时，转变为（0x7D, 0x5E） 2.当data中出现0x7D时，转变为（0x7D, 0x5D） 3.当data中出现ASCII码中的控制字符时（数值小于0x20），转变为（0x7D, data[i]+ 0x20）如0x01转变为0x7D, 0x21</p><h4 id="零比特填充">3.1.3 零比特填充</h4><p>在使用SONET/SDH链路时，使用同步传输（一连串比特连续传送），为实现透明传输，采用零比特填充方法&gt;在信息端出现连续5个1时，发送端在后面加一个0，接收端在接收到连续5个1时，如果后面是0，就丢弃0</p><h3 id="ppp协议的工作状态">3.2 PPP协议的工作状态</h3><ol type="1"><li>链路建立状态，建立链路层的LCP连接发送LCP的配置请求帧，是一个PPP帧，协议字段位LCP对应代码，信息字段为配置请求。另一端可以发送配置回应帧，也可以发送拒绝帧，如果发送拒绝帧，就会重新发送配置请求帧，直到对方发送配置回应帧。结束后建立了LCP链路。</li><li>鉴别状态只能发送LCP协议分组，鉴别协议分组。如使用口令鉴别协议PAP，通信一方发送身份识别符和口令，若鉴别成功，则进入网络控制协议NCP阶段</li><li>网络层协议状态PPP链路两端的网络控制协议NCP根据网络层的不同协议互相交换网络层特定的网络分组。例如是IP协议，则配置IP协议模块时使用NCP中支持的IP协议，IP控制协议IPCP</li><li>网络层配置完成后，进入链路打开状态，两端互相发送分组。由过程可以看到，PPP协议还包含物理层和网络层的内容，如协商网络层协议。</li></ol><h2 id="使用广播信道的数据链路">4. 使用广播信道的数据链路</h2><p><strong>局域网特点</strong>：网络为一个单位所有，地理范围和站点数目有限。<strong>局域网优点</strong>： 具有广播功能，一个站点可以方便的访问全网。<strong>以太网</strong>： 局域网的同义词 <strong>适配器</strong>：计算机与局域网的连接是通过适配器进行的，适配器是主板上的一块网络接口板，又称为网卡。适配器与局域网通过电缆通信，与电脑通过i/o总线以并行的方式进行通信。所以其一个主要作用是对数据进行串行传输和并行传输的转换。适配器在接受和发送时，不使用计算机的cpu<br />在<strong>总线型</strong>局域网使用广播的方式进行链路层通信，需要进行冲突检测，以太网使用CSMA/CD协议进行冲突检测。<strong>星型局域网</strong>：以太网的一种，在旧版本使用集线器组成，在逻辑上可以看作总线局域网，需要利用CSMA/CD协议进行冲突检测，现在使用以太网交换机，不需要冲突检测。<br /><strong>帧格式</strong> ：以太网帧格式，包括帧开始符，目的地址，源地址，类型字段，数据字段，CRC校验字段，帧结束符。‘<strong>扩展以太网</strong>：虚拟局域网VLAN，将一个局域网划分为多个虚拟局域网，每个虚拟局域网是一个广播域，不同的虚拟局域网之间不能互相通信，可以通过交换机进行通信。在帧中加入VLAN标签，交换机根据VLAN标签进行转发。<strong>MAC</strong>地址：以太网中的地址，48位，前24位为厂商识别码，后24位为厂商分配的地址。在设备被生产时就固定，不会改变。在设备接收到帧时，会检查目的地址，如果不是自己的地址，就丢弃帧。但是设备还具有混杂模式，可以接收所有的帧。可以进行网络嗅探。当使用局域网接入互联网时，使用PPPoe协议，将以太网帧封装在PPPoE帧中，然后通过ADSL接入互联网。</p>]]></content>
    
    
    <categories>
      
      <category>计算机网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>将jupyter转换为markdown格式</title>
    <link href="/2024/09/30/%E5%B0%86jupyter%E8%BD%AC%E6%8D%A2%E4%B8%BAmarkdown%E6%A0%BC%E5%BC%8F/"/>
    <url>/2024/09/30/%E5%B0%86jupyter%E8%BD%AC%E6%8D%A2%E4%B8%BAmarkdown%E6%A0%BC%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>需要安装jupyter</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install jupyter<br></code></pre></td></tr></table></figure><p>然后执行以下命令即可将当前目录下的所有ipynb文件转换为markdown格式<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.system(<span class="hljs-string">&quot;jupyter-nbconvert --to markdown *.ipynb&quot;</span>)<br></code></pre></td></tr></table></figure>如果出现jupyter不是内部或外部命令的错误，可以尝试将python的Scripts目录添加到环境变量中</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>jupyter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Domain_Adaptation</title>
    <link href="/2024/09/30/Domain-Adaptation/"/>
    <url>/2024/09/30/Domain-Adaptation/</url>
    
    <content type="html"><![CDATA[<h1 id="domian-adaptation">Domian Adaptation</h1><p>领域适应（DomianAdaptation）是迁移学习的一个子方法，但有不同于迁移学习。</p><p>域适应是使用模型从足够训练集的相关域中训练到的知识，来提高模型再不足训练数据目标域上性能的技术。</p><p><strong>领域适应是为了解决训练模型在验证或者实际数据集上表现不佳的问题</strong></p><p>例如：1.训练数据集是手机拍摄的图片，而验证数据集是单反拍摄的图片，可能就会由于两者的差距而导致模型在验证集上表现不佳。</p><p>2.在MFR问题上，训练采用的是合成遮蔽人脸图像。训练出来的模型在合成人脸训练集上表现好能达到96%的准确率，但是在真实人脸训练集上表现不佳，只有78%的准确率。</p><p>使用领域适应，在一个数据集上寻来你的模型，不需要再新数据上重新训练。</p><h2 id="领域适应相关概念">领域适应相关概念</h2><p><strong>源域：</strong>这是使用标记示例训练模型的数据分布。在上面的示例中，手机照片创建的数据集是源域。</p><p><strong>目标域：</strong>这是一种数据分布，在不同域上预训练的模型用于执行类似的任务。目标域是使用上例中的DSLR 相机的照片生成的数据集。</p><p><strong>领域翻译：</strong>领域翻译是在两个领域之间找到有意义的对应关系的问题。</p><p><strong>域转换：</strong>域转换是模型不同域（例如训练集、验证集和测试集）之间数据统计分布的变化。</p><h2 id="领域适应方法">领域适应方法</h2><ul><li>Supervised DA 监督DA</li><li>Semi-Supervised DA 半监督DA</li><li>Weakly Supervised DA 弱监督DA</li><li>Unsupervised DA 无监督DA</li></ul><h2 id="再mfr问题中应用领域学习论文">再MFR问题中应用领域学习论文</h2><p><ahref="https://www.sciencedirect.com/science/article/abs/pii/S003132032400325X">Maskedface recognition using domain adaptation</a></p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>元学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ROC曲线</title>
    <link href="/2024/09/30/ROC%E6%9B%B2%E7%BA%BF/"/>
    <url>/2024/09/30/ROC%E6%9B%B2%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h1 id="roc曲线">ROC曲线</h1><h3 id="x01-receiver-operating-characteristic-roc-curve.">0x01 receiveroperating characteristic (ROC) curve.</h3><p>T/F（识别结果正确或错误）P/N(模型识别结果)</p><h4 id="section"></h4><p><strong>True Positive Rate</strong>(TPR) （aka，recall 真阳率）</p><p>用于衡量对positive目标的识别准确率的，如果将所有positive目标（理想）/所有目标（极限）识别为positive，则TPR=1.</p><p>理想情况下，模型将坏人识别为坏人的概率</p><p>极限情况下，模型目标是检测坏人时，将所有人识别为坏人，宁可杀一万，不放过一个。</p><p>TRP越高越好</p><p>TP，true positive 正确positive识别为positive，FN ，false negative错误negative识别为positive</p><p>defined as fellows:</p><p><span class="math display">\[TPR = \frac{TP}{TP+FN}\]</span></p><p><strong>False Positive Rate</strong>（FPR）</p><p>用于衡量对negative目标的误报率，如果将所有negetive目标/所有目标识别为positive，则FPR=1</p><p>模型将好人识别为坏人的概率</p><p>FPR越低越好</p><p>defined as fellows: <span class="math display">\[FPR = \frac{FP}{FP+TN}\]</span></p><p><strong>越倾向于将所有目标识别为positive则FPR和TPR，精准度和误报率回同时上升</strong></p><p><strong>ROC curve</strong></p><p>ROC 曲线绘制了不同分类阈值下的 TPR 与FPR。降低分类阈值会将更多项目分类为阳性，从而增加假阳性和真阳性</p><p><strong>classification thresholds（分类阈值）</strong></p><p>分类阈值指将多大概率的可能性识别为真，sigmoid和softmax会输出一个0到1之间的概率</p><p>threshold决定我们是当模型预测概率大于90%还是80%时预测为真。</p><p>大多数算法的 ML 阈值默认设置为 0.5</p><p>在没有适当的模型评估和分析的情况下，假设默认的 0.5分类阈值对于特定用例来说是正确的可能是有风险的。模型错误分类的数量和性质将决定机器学习计划的成功。设置适当的分类阈值对于限制这些错误分类至关重要，因此在机器学习中是不可或缺的。</p><p><strong>分类阈值选择最常用的方法是绘制 ROC 曲线。 ROC代表接收者操作特征，绘制所有分类阈值下的真阳性率 ( TPR= TP TP + FN )和假阳性率 ( FPR= FP FP + TN )</strong></p><p>ROC曲线可以快速直观地了解分类器的准确性。曲线越接近直角，模型越准确。返回<strong>曲线左上角</strong>的分类阈值（最小化TPR 和 FPR 之间的差异）是最佳阈值</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>martix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>元学习</title>
    <link href="/2024/09/30/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/09/30/%E5%85%83%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="一文入门元学习meta-learning附代码---知乎-zhihu.com"><ahref="https://zhuanlan.zhihu.com/p/136975128">一文入门元学习（Meta-Learning）（附代码）- 知乎 (zhihu.com)</a></h1><p>元学习是一种学习超参数的学习方法，区别于一般的机器学习是利用人为设置的超参数更新参数。让模型学会自己初始化参数。</p><p>元学习关注的是该超参数在任意任务下训练后表现的潜力大小。</p><h2 id="malmlmodel-agnostic-meta-learning">MALML（Model-AgnosticMeta-Learning）</h2><p>MAML 算法流程：</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-eda1034966a79b9f1c30dc6527b57830_1440w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>元学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人脸识别中的loss函数</title>
    <link href="/2024/09/30/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84loss%E5%87%BD%E6%95%B0/"/>
    <url>/2024/09/30/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84loss%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="人脸识别中的loss函数">人脸识别中的loss函数</h1><h2 id="ox00-softmax-cross">Ox00 softmax + cross</h2><h3 id="deepface">Deepface</h3><p>--- <a href="loss函数.assets/DeepFace.pdf">论文</a></p><p>在论文最后一个全连接层的输出到一个k-th的softmax，K是类数。softmax会将输入转化为和为1的概率分布。<span class="math display">\[p_k =  \]</span></p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸识别</tag>
      
      <tag>loss</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BN层</title>
    <link href="/2024/09/30/BN%E5%B1%82/"/>
    <url>/2024/09/30/BN%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="batch-normalizationbn">Batch Normalization（BN）</h1><p>见<ahref="https://blog.csdn.net/weixin_44023658/article/details/105844861">BatchNormalization（BN）超详细解析_batchnorm在预测阶段需要计算吗-CSDN博客</a></p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>领域适应在MFR中的应用</title>
    <link href="/2024/09/30/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94%E5%9C%A8MFR%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2024/09/30/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94%E5%9C%A8MFR%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="masked-face-recognition-using-domain-adaptation">Masked facerecognition using domain adaptation</h1><p><ahref="https://www.sciencedirect.com/science/article/pii/S003132032400325X">论文</a>-- 细读 2024.9</p><p>由于缺乏带有身份标签的真实世界蒙面人脸数据集，大多数现有蒙面人脸识别方法直接在从现有未蒙面人脸数据集生成的合成蒙面人脸上重新训练模型[<ahref="https://www.sciencedirect.com/science/article/pii/S003132032400325X#bib0010">10</a>]。这种方法的主要缺点是<strong>使用合成蒙面人脸不能保证人脸识别系统对现实世界蒙面人脸的性能</strong>。</p><h2 id="introduce">introduce</h2><p>该网络使用了1）ResNet50作为网络的主干2）基于自注意力的refinementlayer 3）使用领域对抗学习网络（DANN）来利用合成人脸和真实人脸。</p><p><strong>真实人脸数据只有是否遮挡的标签，而没有身份标签。</strong></p><p>2）refinement layer利用注意力分数损失来对其遮挡和未遮挡人脸的激活区域。能够减少遮挡人脸和未遮挡人脸的类内差距，进而增强鲁棒性。</p><p>3） 领域对抗学习网络用来使真实人脸和遮挡人脸的特征分布一致。</p><p>领域适应在人脸识别领域早有运用，例如人脸识别适应低分辨率图像，适应不同姿势人脸图像。</p><p>source domain -&gt; 未遮挡人脸 target domain -&gt; 遮挡人脸</p><h2 id="普通人脸识别在masked-face-上的表现分析">普通人脸识别在maskedface 上的表现分析</h2><p>人脸识别任务可以分为两类：</p><ul><li>face verification --&gt; 1：1识别即识别人脸对是否匹配</li><li>face identify --&gt; 1:n识别从人脸数据库中找到与输入人脸图片最匹配的图片</li></ul><p>start-of-the-art 人脸识别器使 ArcFace</p><h3 id="pure-arcface">pure ArcFace</h3><p>合成遮挡人脸使利用<em>MaskTheFace</em> 软件生成，即从LFW（LabeldFaces in the Wild）到MLFW。</p><p>模型的训练论文中没有给出，不知道是哪来的预训练模型</p><table><thead><tr><th></th><th>Accuracy</th></tr></thead><tbody><tr><td>LFW</td><td>99.3%</td></tr><tr><td>MLFW</td><td>96.2%</td></tr></tbody></table><p>从激活区域可视化中可以看到，遮挡人脸和未遮挡人脸在激活区域的分布上不同</p><p>未遮挡人脸是整个面部激活，遮挡人脸只有上部是激活的。</p><blockquote><p>意思是要训练之后修改模型对遮挡人脸改为也是上部人脸激活，这样遮挡人脸和未遮挡人脸通过模型获得的表征会更加接近，以此来提高真确率</p><p>这样可能会有一个问题，模型不具备对上部人脸遮挡图像的识别能力。</p></blockquote><p>对提取的特征使用PCA（主成分提取）得到的特征分布并没有完全重叠。</p><h3 id="retrained-arcface---using-synthetically-face">Retrained ArcFace--using synthetically face</h3><p>利用MaskTheFace从 CASIA-WebFace得到MCASIA-WebFace</p><p>然后再MCASIA-WebFace上进行了Retrain</p><p>再MLFW上得到了98.5%的verification正确率，相比于pure有2.3%的提高</p><blockquote><p>:satisfied:高于G2D模型的97.75%，G2D模型的结果居然还没这个的baseline高</p></blockquote><p>从激活区域图中c可以看到对下部区域的激活减弱了。</p><p>对提取的特征使用PCA（主成分提取）得到的特征分布再合成人脸上重合了，但是在真实人脸上没有重合。</p><blockquote><p>PCA:从高维特征中提取到低维特征表示，类似于投影</p></blockquote><h3 id="proposed-method">Proposed method</h3><ol type="1"><li>feature Extraction Backbone 可以是残差网络如ResNet-18，ResNet-50，用来提取特征的</li><li>feature Refinement layer是一个transformer，是用来对其激活区域的</li><li>Identity Classification Layer 是一个分类损失层，可以用ArcFace</li><li>Domain Adaptation Layer 是用来领域适应，特征分布转换。</li></ol><h4 id="feature-refinemnet-layer">2. Feature refinemnet layer</h4><p>用来对激活区域进行对齐来提取关键特征的。是一个自注意力机制块，自注意力分数和value相乘输出</p><p>所以<strong>注意力得分高的就可以代表这个区域是高度激活的</strong>。</p><h4 id="domain-adaptation-layer">4.Domain Adaptation Layer</h4><p>为了解决特征分布不重合的问题</p><p>使用Mask Label训练。</p><p>使用领域对抗（DANN）网络训练（与GAN网络类似），有一个generator和一个discriminator。</p><p>discriminator是要从generator生成的潜在特征中分辨出蒙面特征，是一个分类器，分类该图片是否蒙面，<spanclass="math inline">\(min(L(m))\)</span></p><p>generator是要让潜在特征中的蒙面特征难以被discriminator发现。是一个特征提取器，提取出的特征要让discriminator看不出来是蒙面图像。<spanclass="math display">\[min(-L(m))\]</span> GRL层实现梯度反转</p><p>据此，可以让蒙面和非蒙面特征分布重合。</p><h3 id="loss-function">Loss function</h3><p><span class="math inline">\(x\)</span>表示未蒙面图像，<spanclass="math display">\[\hat{x}\]</span> 表示对应的合成人脸图像，<spanclass="math display">\[x^r\]</span>表示真实人脸图像，<spanclass="math display">\[y\]</span>表示<spanclass="math display">\[X和\hat{X}\]</span>的身份标签，<spanclass="math display">\[y^r\]</span>表示真实人脸的蒙面标签（取值应该是0或1）。</p><h4 id="attention-score-loss">attention score loss</h4><p>注意力分数损失包含注意力分数，通过优化这个损失函数，能够让注意力分数在遮挡和未遮挡人脸上表现相同，从而减少类内差距。<span class="math display">\[L_a  = \frac{1}{B}\sum_{i=1}^B\parallel A(x_i) -A(\hat x_i)\parallel\]</span> 可以看到要利用输入图像对来计算损失函数</p><p>这是优化Feature refinement layer的损失函数</p><h4 id="mask-classificaton-loss">Mask classificaton loss</h4><p>是用来优化GANN网络的损失函数 $$ L_m = L_m^s + 0.5L_m^r\</p><p>L_m^s = -<em>{i=1}^Blog{}+log{}\ L_m^r =-</em>{i=1}<sup>{B}(y</sup>r_i)log{}+(1-y_i^r)log $$可以看到合成图片成对输入，真实图片单独输入</p><p><span class="math inline">\(L_m^s\)</span>计算的是未遮挡人脸提取特征中未遮挡人脸特征占比，和合成遮挡人脸提取特征中遮挡人脸占比。</p><p><span class="math inline">\(L_m^r\)</span>计算的是真实人脸提取特征中，如果是遮挡人脸，遮挡人脸特征占比，反之这是未遮挡人脸占比</p><p>这个Loss和可以反映模型提取了多少正确特征，加上负号翻转。</p><h4 id="identity-classification-loss">Identity classification loss</h4><p>使用ArcFace loss</p><p>也是一种softmax损失计算方式。</p><h2 id="实验">实验</h2><p><strong>数据集</strong>：</p><ul><li>CASIA-WebFace （training）</li><li>MCASIA-WebFace （training）</li><li>RMFD --spilt into two part<ul><li>RMFD_DA (training)</li><li>RMFD_FV (testing)</li></ul></li><li>MFR2 (testing)</li><li>MFRFI (testing)</li></ul><h3 id="部署细节">部署细节</h3><ol type="1"><li>backbone 是完整的ResNet-50</li><li>Feature Refinement layer</li></ol><p>输出是512维的</p><ol start="3" type="1"><li><p>domin adaption layer -- mask classification layer</p><p>输出是一个2维向量，表示masked标签</p><ol start="4" type="1"><li><p>identity classification layer</p></li></ol></li></ol><p>输出应该会接一个softmax计算ArcFace loss</p><h3 id="结果">结果</h3><p>RMFD上的验证结果87.84，相比于STL，retrainedArcFace来说提升不大，可能是由于RMFD中人脸图像质量低，MTCNN无法对人脸进行对其，而导致的图像质量差。</p><p>在MFR2和MFTFI上有很高的正确率</p><h3 id="分析添加的domain-adaptation和refinemen层的影响">分析添加的domainAdaptation和refinemen层的影响</h3><p>可以看到只添加一层和原始层之间的对比关系</p><p>在RMFD上还是只有少量提升</p><h2 id="总结">总结</h2><p>该论文在ArcFace的基础上添加了两层来帮助训练，训练的目的无非就是为了对齐遮蔽和未遮蔽人脸的提取特征，更准确的说应该是普通模型提取的特征向遮蔽人脸靠拢（不管是前部的激活区域，还是后部的特征分布）。让模型在提取参照人脸特征时学会“看到遮蔽时的特征“。</p><p>这种方法可能会减少在识别普通人脸时的准确性，不过这应该不是问题，在具体应用中，应该可以应用准确率更高的检测是否遮挡的模型先判断时遮挡人脸还是未遮挡人脸，如果时遮挡人脸再用遮挡人脸识别模型处理即可。</p><p>既然是为了让参照人脸提取特征向遮蔽人脸靠，那么我们为什么不直接对参照人脸进行预处理，而不修改模型，将待识别图像的遮蔽物合成到参照人脸上，再对参照人脸提取特征，是否会直接地使特征分布更加重合。进一步也可能会减少因不同遮蔽物引起的鲁棒性问题，因为参照人脸的遮蔽物由检测人脸遮蔽物而改变。</p><p>但是，为所有参照人脸合成上遮蔽物而不修改模型的化，可能会引起参照人脸之间的类内差距，依旧可能需要一定的迁移学习。</p><p>也即提高在MR-MP设置下的模型表现，本论文未进行这方面的实验。</p><p>在G2D的实验中，不同模型在MUR-MP和MR-MP中的表现差异没有特定规律，大部分模型在UMR设置下表现更好，少部分模型在MR下表现更好，不知道能不能提升在合成后的MR设置下的性能。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>G2D</title>
    <link href="/2024/09/30/G2D/"/>
    <url>/2024/09/30/G2D/</url>
    
    <content type="html"><![CDATA[<h1id="masked_face_detection-with-genenerative-to-discriminative-representations">masked_face_detectionwith Genenerative-to-Discriminative Representations</h1><p><a href="2405.16761v1.pdf">论文</a></p><p>现在对于遮蔽脸部识别有两种主流的解决方案，分为生成式和识别式模型。</p><p>本篇论文的创新点在于把生成式模型和识别式模型连接了起来，形成了简称G2D的融合模型</p><p>G2D模型先由生成式模型的编码器产生注重类别信息的描述特征（category-awaredescriptors），产生的特征输入到识别器模型中得到身份识别向量（identity-awarevectors）。</p><p>编码器和鉴别器作为神经网络的主干，输出的特征再给到一个分类器最终能够识别身份信息。</p><p>总的来说，G2D模型是由生成式编码器（generativeencoder）、识别改装器（discriminative reformer ）、特征分类器（ FeatureClassifier）顺序连接形成的模型，能够融合生成式和识别式模型的优点。</p><blockquote><p>首先，生成式编码器采用预训练好的人脸绘制模型的编码器，将被遮挡的人脸表示为具有丰富被遮挡人脸一般信息的类别感知描述符，以区分人脸与其他物体;然后，判别改造器结合了一个22层卷积网络，并学习将类别感知描述符转换为身份感知向量以增强识别。最后，特征分类器级联一个全连接层和一个softmax层来识别改造后的向量。该方法将生成式编码器和判别式重构器结合在一起，作为被遮挡人脸特征提取的主干，逐步进行自监督预训练，特征分类器作为识别头。最后，对骨架进行冻结，并对标记好的被遮挡面进行特征分类器的微调。</p><p>具有三个模型，分别为生产式编码器（人脸绘制模型的一部分），提取出人脸类别感知符的表征。判别改造器用卷积神经将感知符转换回身份识别向量来加强身份识别能力，最后将识别向量导入身份识别器。</p></blockquote><h2 id="生成式编码器-generative-encoder">生成式编码器 （GenerativeEncoder)</h2><h3 id="生成式模型概念">生成式模型概念</h3><p>生成式模型的思想是对遮挡部分进行重绘得到完整人脸，然后进行面部识别。</p><p>对于一般的生成式模型论文作者提出：</p><blockquote><p>由于在忽视了正则化和身份保留，在图像填补时会丢失<strong>内在和外在的识别关系</strong>，一些试图加强身份信息保留的方法帮助有限的</p></blockquote><p>即补全生成的图片身份识别效果不好。</p><p>相关论文:<ahref="Two-Stream_Prototype_Learning_Network_for_Few-Shot_Face_Recognition_Under_Occlusions.pdf">Two-Stream_Prototype_Learning_Network_for_Few-Shot_Face_Recognition_Under_Occlusions.pdf</a></p><h3id="g2d模型中的生成式编码器的选取">G2D模型中的生成式编码器的选取</h3><p>生成式编码器负责从被遮挡区域中提取一般的面部特征。</p><p>G2D模型中的编码器是ICT模型，一种最新的基于transfromer的图片补全模型。ICT模型包含一个transformer网络提取特征根和一个CNN网络上采样，G2D模型的编码器的输出特征来自于ICT模型上采样网络中的一个残差块。</p><p><ahref="https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_High-Fidelity_Pluralistic_Image_Completion_With_Transformers_ICCV_2021_paper.pdf">Wan,Z., Zhang, J., Chen, D., and Liao, J. High-fidelity pluralistic imagecompletion with transformers. In ICCV, pp. 4692–4701, 2021.</a></p><h2 id="识别器重构器-discriminative-reformer">识别器重构器(discriminative reformer )</h2><h3 id="识别器概念">识别器概念</h3><p>通过减少遮蔽区的影响来提取鲁棒性特征。一些论文想通过构建更加强大的神经网络来从未遮挡区域提取出更多的信息</p><p>作者提出：</p><blockquote><p>1.直接微调现有的识别模型会增加对遮蔽脸的识别能力但会减少对未遮蔽脸的识别能力。</p><p>2.对于不同种类的遮蔽类型可能会导致鲁棒性弱的特点。即对于相同人脸不同口罩的识别缺少一致性。</p></blockquote><p>相关论文：<ahref="Masked_Face_Transformer.pdf">Masked_Face_Transformer.pdf</a></p><h3 id="g2d模型中选用的识别器">G2D模型中选用的识别器</h3><p>选用一个叫做Resnet-like的网络作为识别重构器，它由一个卷积层、4个残差块、一个池化层和一个全连接层组成。作者实验发现网络越深重构效果越好，但会大大增加模型复杂度。</p><p>在训练时以一个预训练的通用人脸识别器为老师，通过知识蒸馏来指导生成到判别表示的转换。</p><h2 id="特征分类器">特征分类器</h2><p>特征分类器从改进的判别表示中预测人脸身份。它表现为一个简单的分类头，具有一个完全连接的层和一个softmax层</p><p>分类器，只能对k类人脸进行区分。（我不清楚训练这个分类器的意义在哪里</p><p>:question:</p><p>要是在推理时用分类器，那么不是每录入一张人脸就要重新训练分类器，增加分类器输出维数吗。</p><p>事实上大多数face recognition用的是度量学习任务而不是分类任务。</p><p>通过利用特征向量余弦距离来判断match的可能性，而不是分类中softmax的最大值。</p><p>采用端到端的方式进行训练分类器参数</p><h2 id="实验及结果分析">实验及结果分析</h2><p>实验评价主要在人工合成和真实遮挡人脸两个训练集上进行</p><h3 id="验证训练集">验证训练集</h3><ul><li><p>Celeb-A -- 作为人工合成遮蔽人脸的训练数据集</p></li><li><p>LFW -- 作为人工合成遮蔽人脸的验证数据集</p></li><li><p>RMFD&amp;MLFW --真实世界遮蔽人脸的验证数据集，只使用其中真实人脸图像验证基准</p></li></ul><h3 id="验证基准">验证基准</h3><p><strong>四种对比基准</strong>:</p><ol type="1"><li>四个常用基础人脸识别器</li></ol><ul><li>CenterLoss（CL）</li><li>VGGFace (VGG)</li><li>VGGFace2 (VGG2)</li><li>ArcFace(AF)</li></ul><ol start="2" type="1"><li>选用四种人脸补全方法，将图片通过人脸补全方法预处理在输入进人脸识别器<ul><li>GFC</li><li>DeepFill</li><li>IDGAN</li><li>ICT</li></ul></li><li>微调后的遮蔽人脸识别器</li><li>在遮蔽人脸数据集上从头开始训练的模型</li></ol><p><code>2</code>是生成式人脸识别器，<code>3</code> <code>4</code>是识别式人脸识别器， <code>4</code>采用的是DoDGAN模型。所用的模型均采用达到是其发行的预训练模型。</p><p><strong>评价：</strong></p><ol type="1"><li><p>MR-MP，蒙面参考与蒙面检测</p></li><li><p>MUR-MP，非蒙面参考与蒙面检测</p></li></ol><p><strong>指标</strong>：准确率 (ACC)、等错误率 (EER)、Fisher 判别比(FDR)、错误匹配率 (FMR)、错误不匹配率 (FNMR)、FMR ≤ 1.0% 的最低 FNMR(FMR100)、FMR ≤ 0.1% 的最低 FNMR (FMR1000)，以及基于 FMR100 Th 和FMR1000 Th 阈值计算的平均值 (AVG)</p><p>对于 FMR （false match rate（系统错误地将两种不同的输入认为是匹配的概率） ）和 FNMR （false matchrate （系统错误地将两个匹配的输入认为是不匹配的概率））</p><h3 id="人工合成遮蔽人脸测试">人工合成遮蔽人脸测试</h3><p>采用利用LFW图片合成的遮蔽图片，由3000个正确匹配对，和3000个不正确匹配对。图片合成</p><h3 id="基于1-2-基准的对比">基于<code>1</code> <code>2</code>基准的对比</h3><p>G2D模型的ACC能够达到97.58%的准确率。</p><p>对于 FMR （false match rate（系统错误地将两种不同的输入认为是匹配的概率） ）和 FNMR （false matchrate （系统错误地将两个匹配的输入认为是不匹配的概率））</p><h4 id="结论">结论</h4><ul><li>首先，不同的掩码导致准确率明显下降，这与之前的研究结果一致（Ngan etal.，2020a;b）</li><li>其次，生成式人脸修复有时并不总是能够填补这一空白。我们注意到，VGG2和 GFC 的组合比单独的 VGG2实现的准确率更低，这表明如果修复过程不能正确地进行正则化，它可能会发挥负面作用。我们怀疑这是由于相似掩码模式的干扰和修复模型的鲁棒性较差造成的。</li><li>第三，在人脸修复加识别范式中，采用修复方法确实会对复合模型的性能产生影响。此外，在合成掩码LFW 上，IDGAN 在 48×48 掩码下实现了 96.53% 的准确率（Ge etal.，2020），而我们的 G2D 即使在更复杂的掩码下也能达到97.58%。最后，我们的 G2D 优于所有组合，证明了我们方法的有效性。</li></ul><h3 id="基于3-4-基准的对比">基于<code>3</code> <code>4</code>基准的对比</h3><p>采用了两种修复方法 DeepFill 和 ICT与四个识别器的组合，以及两个最近提出的蒙面人脸识别模型 DoDGAN (Li etal., 2020) 和 Self-Restrained Loss (SRT) (Boutros et al.,2022)，以进行更定量的比较。在这里，我们不采用 IDGAN，因为它与 DeepFill共享相同的主干，并经过完全身份监督的训练。我们打算更多地关注自监督表征学习的有效性。</p><p>对于 SRT，报告了两个基线（ResNet50和 MobileFaceNet）以及使用 SRT损失训练的额外模块的性能。</p><p>SRT --损失函数旨在通过引入自我约束机制来增强模型的泛化能力和鲁棒性</p><p>如表 1 所示，对于在顶部添加额外模块对现有深度识别器进行微调的SRT，原始基线（而不是改进后的基线）表现出更好的性能。这表明，尽管这些解决方案可以在蒙版样本上恢复一些性能，但深度模型的泛化能力很容易受到影响。同样，最近的工作DoDGAN（Li etal.，2020）在交叉质量评估中经历了明显的下降。本质上，这些方法不能适当地处理潜在空间中蒙版和非蒙版样本之间的分布差异。我们的G2D 在 MR-MP 和 UMR-MP 设置上都实现了最高的准确率</p><p>FMR 和 FNMR 结果分析值得注意的是，基于现成人脸识别器的方法显示出较低的错误匹配率(FMR),这表明它们倾向于将更多的正对 (具有相同身份)预测为负对，而对负对的预测受影响较小,这揭示了我们动机上的根本区别。当发生遮挡时，对于一般的人脸识别器来说，主要的挑战是预先存在的类内特征的无效化。我们的方法则不同，它教会模型怀疑和重新校准。还值得注意的是，我们的模型呈现出较低的FMR 和 FNMR 平均值，尤其是在 UMR-MP 设置下。这表明我们提出的 G2D 在 FMR和 FNMR 之间实现了更好的平衡,对于遮蔽和未遮蔽的有更好的泛化识别能力</p><h3 id="真实遮挡人脸测试">真实遮挡人脸测试</h3><p>在RMFD数据集上测试</p><p>可以看到在真实数据下人脸识别的正确率都有大幅下降。</p><p>具有 SRT 的模型在 FMR 和 FNMR之间表现出极端不平衡，这表明它们对蒙面人脸识别场景过度拟合，同时几乎完全牺牲了对未蒙面人脸的判别。相反，我们的G2D 表现出更好的连接未蒙面和蒙面人脸的能力，这在实际应用中相当有价值</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>SSDMNV2</title>
    <link href="/2024/09/30/SSDMNV2/"/>
    <url>/2024/09/30/SSDMNV2/</url>
    
    <content type="html"><![CDATA[<h1 id="ssdmnv2">SSDMNV2</h1><p>人脸检测模型，其框架为：SSD将人脸框出再利用mobileNetV2对MASKED进行二分类。</p><p><strong>人脸检测</strong>: 检测一个人脸是否佩戴口罩</p><p>opencv 的SSD（single shot multiboxdetector）是一个以ResNet-10为basebone的模型</p><p>MobileNet是一个图形模型再pytorch，TensorFlow中有预训练权重。加入分类头做迁移学习分辨是否带口罩。</p><p><strong>SSD</strong>： 读取一张图片检测出多个目标</p><p>该莫i选哪个的主要特点在于可以达到15.71的FPS，相比于VGG，ResNEt-50，FPS高，</p><p>鉴定为水文，但是引用量很高。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Masked_face综述</title>
    <link href="/2024/09/30/Masked-face%E7%BB%BC%E8%BF%B0/"/>
    <url>/2024/09/30/Masked-face%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="masked-face-recognize">masked face recognize</h1><h3 id="最新综述">最新综述：</h3><p><a href="https://arxiv.org/abs/2405.05900">A Comprehensive Survey ofMasked Faces: Recognition, Detection, and Unmasking</a> 2024<imgsrc="Masked-face综述\image-20240811111817098.png"alt="image-20240811111817098" /></p><ul><li>FU (face unmasking) -- 去除遮蔽物的方法，generative 方法</li><li>FMR(Face Mask Recognition) -- 分辨是否带口罩的方法</li><li>MFR(Masked Face Recognition) --遮蔽人脸的识别</li></ul><p>MFR有三个主要方法</p><ul><li><p>整体方法</p><p>整体方法采用深度学习模型来识别整个面部的特征，并利用注意力模块</p></li><li><p>基于掩模排除的方法</p><p>基于掩模排除的方法训练模型识别未掩模的面部特征，例如眼睛和头部。</p></li><li><p>基于蒙版去除的方法</p><pre><code class="hljs">  利用生成对抗网络 (GAN) 从蒙版对应物中创建逼真的面部图像</code></pre></li></ul><h3id="人脸识别在现实世界的复杂性调查">人脸识别在现实世界的复杂性调查</h3><p>论文：<a href="A%20Survey%20on%20Occluded%20Face%20recognition">ASurvey on Occluded Face recognition</a> .2020</p><p>探究了在现实世界中人脸识别复杂性的情况。</p><h3 id="rmfd-真实人脸数据集">RMFD 真实人脸数据集</h3><p><ahref="https://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Huang_Masked_Face_Recognition_Datasets_and_Validation_ICCVW_2021_paper.pdf">Huang_Masked_Face_Recognition_Datasets_and_Validation_ICCVW_2021</a>武大实验室</p><p>在真实面罩数据集领域，真实世界蒙面人脸识别数据集（RMFRD)脱颖而出，成为最广泛的MFR 公开可用资源之一。</p><h3 id="合成人脸数据">合成人脸数据</h3><p>然而，由于大量可用于生成目的的公共人脸数据集，合成蒙面人脸数据集的可访问性得到了显着提升。与具有真实面具的数据集相比，合成面具数据集通过结合人工生成的面部覆盖物带来了独特的视角。这些数据集提供了一个受控环境，使研究人员能够探索各种合成面具的变化，包括样式、颜色和形状等考虑因素。这些数据集的受控性质有助于系统地探索MFR 中的挑战，为模型对不同合成掩模的响应提供有价值的见解。</p><p>Z. Wang、B. Huang 等人[ <ahref="https://arxiv.org/html/2405.05900v1#bib.bib42">42</a>]采用了不同的方法，将掩模自动应用于来自现有公共数据集（例如CASIA-WebFace [ <ahref="https://arxiv.org/html/2405.05900v1#bib.bib23">23</a> ] 、LFW [ <ahref="https://arxiv.org/html/2405.05900v1#bib.bib25">25</a> ] 、CFP-FP [<a href="https://arxiv.org/html/2405.05900v1#bib.bib57">57</a>]）的人脸图像。和 AgeDB-30 [ <ahref="https://arxiv.org/html/2405.05900v1#bib.bib58">58</a> ]。这项工作最终创建了一个模拟面具人脸识别数据集 (SMFRD)，其中包含代表16,817 个独特身份的 536,721 个蒙面人脸。 Aqeel Anwar 和 ArijitRaychowdhury [ <ahref="https://arxiv.org/html/2405.05900v1#bib.bib26">26</a> ]介绍了MaskTheFace，这是一种开源工具，旨在有效地掩盖公共人脸数据集中的人脸。该工具用于生成大型蒙面人脸数据集</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸识别</tag>
      
      <tag>遮蔽</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Arcface</title>
    <link href="/2024/09/30/Arcface/"/>
    <url>/2024/09/30/Arcface/</url>
    
    <content type="html"><![CDATA[<ol type="1"><li>用DCNN来学习人脸表示，把人脸图片映射到特征空间，使得类内距离小类间距离大</li><li>一般两条主线方法，代表分别为softmax loss和tripletloss,但是都有一些缺陷</li></ol><h2 id="softmax-classifier">softmax classifier</h2><p>通过多分类问题来训练模型</p><p>缺点如下：</p><blockquote><ol type="1"><li>线性变换矩阵的大小W∈Rd×n随着身份数的增加而线性增加n;</li><li>学习到的特征对于封闭集的分类问题是可分离的的分类问题是可分离的，但对于开放性的人脸识别问题来说，却没有足够的鉴别力。脸部识别的问题。</li></ol></blockquote><p>softmax损失函数并没有明确地对特征嵌入进行优化。特征嵌入，对类内样本执行更高的相似性，对类间样本执行更高的多样性。这导致了在大的类内外观变化（如姿势变化）下，深度人脸识别的性能差距。大的类内外观变化（例如：姿势变化[28, 44]和年龄差距[19,45]）和大规模测试场景下的性能差距。(例如，百万[12, 37,18]或万亿对[1]）。</p><h2 id="triplet-loss">triplet loss</h2><p>通过直接学习及嵌入特征来分类。</p><p>有非常好的效果，但</p><p>缺点如下：</p><blockquote><p>（1）脸的数量会出现组合式爆炸，特别是对于大规模的数据集来说。特别是对于大规模的数据集，会导致迭代步骤的显著增加；（2）半硬样本挖掘是一个相当困难的问题，需要有效的模型训练是一个相当困难的问题。</p></blockquote><p>conflict see this--(https://www.researchgate.net/figure/Conflict-between-triplet-loss-and-softmax-loss-a-f-I-a-f-I-p-f-I-n-are_fig1_337519425)</p><p>see this for more detail --<ahref="https://zhuanlan.zhihu.com/p/34404607">人脸识别的LOSS（上） - 知乎(zhihu.com)</a></p><h1 id="arcface">ArcFace</h1><p>为了拥有边界效益，同时防止在Triplet中样本采集问题，近期论文采用边界处罚</p><p>Arcface: Additive angular margin loss for deep face recognition</p><h2 id="introduce">introduce</h2><p>在softmax过程中linear层的变换矩阵的每一行都可以被视为一个类的中心特征表示</p><p>所以特征与足迹有一个全连接层的点积等于特征与中心归一化后的余弦距离。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>caps映射为esc</title>
    <link href="/2024/09/30/caps%E6%98%A0%E5%B0%84%E4%B8%BAesc/"/>
    <url>/2024/09/30/caps%E6%98%A0%E5%B0%84%E4%B8%BAesc/</url>
    
    <content type="html"><![CDATA[<h2 id="参见link">参见<ahref="https://orxing.top/post/d3c3145e.html#%E5%89%8D%E8%A8%80">link</a></h2>]]></content>
    
    
    <categories>
      
      <category>Windows</category>
      
    </categories>
    
    
    <tags>
      
      <tag>便捷操作</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文件权限</title>
    <link href="/2024/09/30/%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/"/>
    <url>/2024/09/30/%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/</url>
    
    <content type="html"><![CDATA[<h1 id="文件权限详解">文件权限详解</h1><p><strong>使用ls -l查看文件详情</strong>  字段说明 文件权限字段说明<br />## 文件类型字符 - 当为[ d ]则是目录，例如上表文件名为“.config”的那一行；- 当为[ - ]则是文件，例如上表文件名为“initial-setup-ks.cfg”那一行； -若是[ l ]则表示为链接文件（link file）； - 若是[ b]则表示为设备文件里面的可供储存的周边设备（可随机存取设备）； - 若是[ c]则表示为设备文件里面的序列埠设备，例如键盘、鼠标（一次性读取设 备）。 -若是[ s ]则表示为socket文件 ## 文件权限三个一组第一组为user，第二组为group，第三组为others - [r]表示可读(read)- [w]表示可写(write)，但不包含删除权限 - [x]表示可执行(execute) ##目录权限 - [r] 表示具有读取目录结构列表的权限,即ls命令 - [w]表示具有更改目录结构列表的权限,即在该目录下创建或删除文件,文件改名，文件移动- [x] 表示具有进入该目录的权限,即cd命令 ::: tip需要目录的x权限才能读取，修改，执行目录的文件，w权限可以删除，没有r权限只是不能ls查看目录文件。## 改变权限命令 ### <code>chgrp</code>改变文件所属群组 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">chgrp [-R] dirname/filename<br><span class="hljs-meta prompt_"># </span><span class="language-bash">-R 递归更改文件或目录的所属群组</span><br></code></pre></td></tr></table></figure> ###<code>chown</code>改变文件拥有者 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">chown [-R] user:group dirname/filename<br><span class="hljs-meta prompt_"># </span><span class="language-bash">-R 递归更改文件或目录的拥有者或者所属群组</span><br></code></pre></td></tr></table></figure> ###<code>chmod</code>改变文件权限 #### 数字方法改变 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">chmod [-R] xyz dirname/filename<br><span class="hljs-meta prompt_"># </span><span class="language-bash">x,y,z为数字，代表三组用户的权限</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">是rwx对应的二进制数</span><br></code></pre></td></tr></table></figure> ####符号方法改变 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"> chmod [-R]| u g o a | +（加入） -（除去） =（设置） | r w x | 文件或目录 |<br><span class="hljs-meta prompt_"># </span><span class="language-bash">u 表示用户，g 表示群组，o 表示其他人，a 表示所有人</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">example</span><br>chmod u+x,a=rw,o-r filename<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目录结构,FHS标准</title>
    <link href="/2024/09/30/%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84-FHS%E6%A0%87%E5%87%86/"/>
    <url>/2024/09/30/%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84-FHS%E6%A0%87%E5%87%86/</url>
    
    <content type="html"><![CDATA[<h1 id="目录fhs标准">目录FHS标准</h1><p>根据FHS的标准文件指出，他们的主要目的是希望让使用者可以了解到已安装软件通常放置于那个目录下，所以他们希望独立的软件开发商、操作系统制作者、以及想要维护系统的使用者，都能够遵循FHS的标准。也就是说，FHS的重点在于规范每个特定的目录下应该要放置什么样子的数据而已。这样做好处非常多，因为Linux操作系统就能够在既有的面貌下（目录架构不变）发展出开发者想要的独特风格。 FHS所定义的三层主目录为：/,/var, /usr三层而已 ## FHS标准的目录结构 - / （root）：根目录； &gt;FHS标准建议：根目录（/）所在分区应该越小越好，且应用程序所安装的软件最好不要与根目录放在同一个分区内，保持根目录越小越好。如此不但性能较佳，根目录所在的文件 系统也较不容易发生问题。 - /usr（unix software resource）：与软件安装/执行有关； - /var（variable）：与系统运行过程有关； - /etc（配置文件）：系统主要的配置文件都放在这里,如人员的账号密码(/etc/password),服务器启动资料,/etc/opt存放第三方软件/opt的配置文件；- /bin（binary）：系统有很多放置可执行文件的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。在/bin下面的指令可以被root与一般帐号所使用，主要有：cat, chmod, chown,date, mv, mkdir, cp, bash等等常 用的指令。 - /sbin （super userbinary）：超级用户的二进制文件,一般是开机过程中需要的命令,只有root才能执行；- usr/sbin 服务器软件程序 - /usr/local/sbin本机自行安装软件所产生的系统可执行文件,如fdisk,fsck,ifconig. - /boot（boot）：开机会用到的文件比如grub2开机管理程序 - /lib（library）：开机会用到的函数库,以及/bin或/sbin会用到函数库； - /home（home）：用户的家目录；~表示当前用户的家目录； - /root（root）：root用户的家目录； - /tmp （temporary）：临时文件； - /dev（device）：设备文件,比如/dev/sda1,/dev/null,/dev/zero - /proc（process）：虚拟文件系统,放置的数据都是在内存当中的,例如系统核心,进程信息不占用硬盘空间,如/proc/cupinfo,/proc/dma,/proc/ioports,/proc/net/等；- /sys（system）：虚拟文件系统记录核心和系统硬件相关的信息,如/sys/block,/sys/bus,/sys/class,/sys/dev等；- /media （media）：软盘,光盘,DVD都会被挂载在这个目录下; - /mnt（mount）：临时挂载点暂时挂载额外设备； - /opt（optional）：第三方软件放置的目录,额外的非分发版提供的软件可以安装在这个目录下,也可以安装在/usr/local下；- /run （run）：系统启动后,程序运行时的信息； - /srv（service）：服务启动后,网络服务启动后,服务取用的数据目录,如网页服务数据放在/srv/www/中；## usr目录的意义与内容FHS建议/usr里防止数据属于可分享与不可变动，所有软件开发者应该将他们的数据放置到这个目录下的次目录。-/usr/bin：和/bin是一模一样的，/bin是链接到/usr/bin的，/bin是为了在单人维护模式下还能够被操作的指令；- /usr/lib和/lib是一模一样的，/lib是链接到/usr/lib的，/lib就是/usr/lib的软链接； - /usr/sbin 同被sbin链接 - /usr/local系统管理员安装的软件建议安装到这个目录 - usr/share主要是只读文件/usr/share/man：线上说明 文档/usr/share/doc：软件杂项的文件说明 /usr/share/zoneinfo：与时区有关的时区文件 - usr/src 源码文件 ## /var 目录的意义/var目录主要针对常态性变动的文件，包括高速缓存（cache）、登录文件 （logfile）以及某些软件运行所产生的文件， 包括程序文件（lock file, runfile），或者例如 MySQL数据库的文件等等 常见次级目录 - /var/cache应用程序本身运行中的一些暂存盘 - /var/lib应用程序的数据文件如，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去- /var/lock 链接到/run/lock 锁定设备或者文件资源 - /var/log 系统日志文件- var/mail 邮件信箱 - var/run 系统启动后,PID目录，链接到/run</p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>磁盘分区与挂载</title>
    <link href="/2024/09/30/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E4%B8%8E%E6%8C%82%E8%BD%BD/"/>
    <url>/2024/09/30/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E4%B8%8E%E6%8C%82%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<h1 id="硬盘的分区格式化和挂载">硬盘的分区、格式化和挂载</h1><ol type="1"><li>对磁盘分区，创建可用partition（类比windows C，D盘符）</li><li>对该partition格式化，创建filesystem （类比windows NTFS）</li><li>创建挂载点</li></ol><h2 id="观察磁盘分区状态">观察磁盘分区状态</h2><p><strong>常用命令</strong> - <code>lsblk</code> 查看磁盘分区状态(listblock devices) - 分区工具 <code>fdisk</code> <code>gdisk</code><strong>example input</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fdisk /dev/vdb<br></code></pre></td></tr></table></figure> ::: info MBR 分区表请使用fdisk 分区， GPT 分区 表请使用 gdisk 分区 ::: - <code>blkid</code>查看设备UUID等参数 - <code>parted</code> 列出磁盘的分区表类型与分区信息<strong>example input</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">parted /dev/vdb <span class="hljs-built_in">print</span><br></code></pre></td></tr></table></figure></p><h2 id="磁盘分区步骤">磁盘分区步骤</h2><ul><li>通过 lsblk 或 blkid 先找到磁盘</li><li>再用 parted /dev/xxx print 来找出内部的分区表类 型</li><li>才用 gdisk 或 fdisk 来操作系统</li></ul><h2 id="创建文件系统">创建文件系统</h2><p><code>mkfs</code> make filesystem CentOS 7 使用 xfs 作为默认文件系统<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkfs.xfs /dev/vdb1<br></code></pre></td></tr></table></figure></p><h3 id="文件系统挂载">文件系统挂载</h3><p>将文件系统挂载到目录上，目录是文件系统的入口 ::: warning -单一文件系统不应该被重复挂载在不同的挂载点（目录）中； -单一目录不应该重复挂载多个文件系统； -要作为挂载点的目录，理论上应该都是空目录,否则原文件系统下的东西会隐藏::: - <code>mount</code> 挂载文件系统 <strong>example input</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/vdb1 /mnt<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>磁盘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell配置</title>
    <link href="/2024/09/30/shell%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/09/30/shell%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="shell-美化">shell 美化</h1><p>用户shell配置文件为<code>~/.bashrc</code>，系统shell配置文件为<code>/etc/bash.bashrc</code>。## 命令提示符 用户名 主机名 - 当前工作目录的完整路径 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim ~/.bashrc<br><span class="hljs-meta prompt_"># </span><span class="language-bash">修改命令提示符格式为</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">┌──(jfs㉿kali)-[~/Desktop]</span> <br><span class="hljs-meta prompt_">#</span><span class="language-bash">└─$</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">添加</span><br>[export] PS1=&quot;\[\e[0;32m\]┌──(\[\e[0;35m\]\u\[\e[0;32m\]㉿\[\e[0;35m\]\h\[\e[0;32m\])-\[\e[0;34m\][\w]\[\e[0;32m\]\n└─$\[\e[0m\] &quot;<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux基础</tag>
      
      <tag>美化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>bash常用热键</title>
    <link href="/2024/09/30/bash%E5%B8%B8%E7%94%A8%E7%83%AD%E9%94%AE/"/>
    <url>/2024/09/30/bash%E5%B8%B8%E7%94%A8%E7%83%AD%E9%94%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="重要热键">重要热键</h1><h2 id="tab-补全"><code>tab</code> 补全</h2><ul><li>在第一个字符串时两次<code>tab</code>显示前缀命令，若只有一个指令则直接补全</li><li>在第二个字符串两次<code>tab</code> 补全文件名'</li><li>若是在--后面两次<code>tab</code> 补全参数 ## <code>ctrl</code>+'c'终止当前命令 ## <code>ctrl</code>+<code>d</code> 退出当前终端</li></ul>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>便捷操作</tag>
      
      <tag>linux基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>仓库里包含仓库</title>
    <link href="/2024/09/30/%E4%BB%93%E5%BA%93%E9%87%8C%E5%8C%85%E5%90%AB%E4%BB%93%E5%BA%93/"/>
    <url>/2024/09/30/%E4%BB%93%E5%BA%93%E9%87%8C%E5%8C%85%E5%90%AB%E4%BB%93%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h1id="当你add的文件夹里面包含了一个git仓库时">当你add的文件夹里面包含了一个git仓库时</h1><p>你会收到以下警告</p><div class="warning"><p>You've added another git repository inside your current repository.Clones of the outer repository will not contain the contents of theembedded repository and will not know how to obtain it. If you meant toadd a submodule, use: git submodule add src If you added this path bymistake, you can remove it from the index git rm --cached src See "githelp submodule" for more information. :::<br />你添加的文件是一个git仓库，这要会导致添加的文件夹不包含文件的内容。 #解决方法 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 删除提交的文件夹</span><br>git <span class="hljs-built_in">rm</span> --cached &lt;folder&gt;<br><span class="hljs-comment"># add</span><br>git add .<br><span class="hljs-comment"># 提交</span><br><br>git commit -m <span class="hljs-string">&quot;Remove the submodule&quot;</span><br><span class="hljs-comment"># 建议重新建一个文件夹，然后将原仓库的文件复制到新文件夹，直接删除后复制整个文件夹可能会出错。</span><br><br></code></pre></td></tr></table></figure></p><h2 id="a-solution-on-the-internet">a solution on the internet</h2><p>solution :https://stackoverflow.com/questions/56873278/how-to-fix-error-filename-does-not-have-a-commit-checked-out-fatal-adding</p></div>]]></content>
    
    
    <categories>
      
      <category>git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>解决linux网络接口消失的问题</title>
    <link href="/2024/09/29/%E8%A7%A3%E5%86%B3linux%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2024/09/29/%E8%A7%A3%E5%86%B3linux%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="x00-恶心的网络问题--可能错怪vm了">0x00恶心的网络问题--可能错怪VM了</h2><p>在vmware虚拟机上的linux系统，有时明明用着一直有网，可突然有一次开机就没网络，之后无论怎么重启都无法使网络连接上。<br />之前我一直以为是vmware的问题，一直在vmware上折腾，来回设置桥接，NAT，但是无果。最后只能通过重装虚拟机或者整个vmware来解决。可能这是一个很简单的问题，刚好这学期加强了一下计算机网络的知识才突然意识到</p><h2 id="x01-查看ifconfig">0x01 查看ifconfig</h2><p>今天启动虚拟机，通过vscode的ssh插件连接上虚拟机，发现网络又不通了。在虚拟机上查看ifconfig<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255<br>        ether 02:42:91:30:75:74  txqueuelen 0  (以太网)<br>        RX packets 0  bytes 0 (0.0 B)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 0  bytes 0 (0.0 B)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536<br>        inet 127.0.0.1  netmask 255.0.0.0<br>        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;<br>        loop  txqueuelen 1000  (本地环回)<br>        RX packets 760  bytes 55418 (55.4 KB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 760  bytes 55418 (55.4 KB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br></code></pre></td></tr></table></figure>发现只有两个网卡，一个是docker0，一个是lo，没有ens33，这就是问题所在。不是网线没接上，而是我网卡掉了啊。<br /><strong>而且这个掉网卡似乎还不能通过重启计算机来解决</strong><br />说好重启解决90%的问题呢</p><h2 id="x02-解决办法">0x02 解决办法</h2><p>于是查找linux网卡掉了怎么办，发现可以通过以下命令来解决 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo service NetworkManager stop <br> <br>sudo rm /var/lib/NetworkManager/NetworkManager.state <br> <br>sudo service NetworkManager start<br><br></code></pre></td></tr></table></figure>重启NetworkManager服务，轻松解决，至于为什么会掉网卡，我也不知道，大概是因为关机姿势不对吧。<br />而且我解决问题之后想去看看这个NetworkManager.state文件到底是个什么东西，发现里面是乱码，是系统重启后自动生成的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>操作系统保护模式</title>
    <link href="/2024/09/29/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BF%9D%E6%8A%A4%E6%A8%A1%E5%BC%8F/"/>
    <url>/2024/09/29/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BF%9D%E6%8A%A4%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="保护模式">保护模式</h1><p>cpu有两种工作模式：实模式和保护模式。当我们打开cpu的时候，cpu是在实模式下工作的，经过某种机制，才进入到保护模式。在保护模式下，cpu有更好的寻址能力，为32位操作系统提供服务。</p><h2 id="从16位到32位操作系统">从16位到32位操作系统</h2><p>在16位操作系统下，寻址是依靠段加偏移的，从而达到1MB的寻址能力，段时地址的一部分，由cs，ds等寄存器保存。<br />但是在32位操作系统下，一个寄存器就有4GB的寻址能力，但是段还是被保留了下来，不过段值寄存器表示的不再是段值，而是一个索引，这个索引只想一个数据结构表的表项。这个表就是GDT（globaldescriptor table）全局描述符表。</p><h2 id="gdt">GDT</h2><p><strong>GDT</strong>：全局描述符<strong>表</strong>，存放段描述符，每个段描述符占8字节，共8192个段描述符，每个段描述符对应一个段，段的大小由段描述符的基地址和限长决定。</p><p><strong>Descriptor</strong>：描述符，是GDT中的一个表项。</p><p><strong>GDTPtr</strong>：GDT指针，是一个48位的寄存器，高16位是GDT的限长，低32位是GDT的基地址。<em>既然如此，我们如何将段寄存器和描述符对应起来呢</em></p><p>答案是通过选择子（selector）</p><p><strong>Selector</strong>：选择子，是一个16位的寄存器，高13位是索引，低3位是RPL（请求特权级），RPL是用来判断当前进程的权限，0是最高权限，3是最低权限。索引是在GDT中的一个偏移。如<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asm">SelectorVideo equ LABEL_VIDEO - LABEL_GDT<br></code></pre></td></tr></table></figure></p><h2 id="从实模式到保护模式">从实模式到保护模式</h2><h3 id="设置gdt">1. 设置GDT</h3><p>将需要的物理地址赋值给到段描述符中。|初始化descriptor<br />将GDT物理地址赋值给GDtPtr数据结构， 初始化GDTPtr然后将GdtPtr中的数据加载到gdtr寄存器中。| 初始化gdtr</p><h3 id="关闭中断">2. 关闭中断</h3><p>因为保护模式下中断处理机制是不同的所以要关中断。</p><h3 id="打开a20地址线">3. 打开A20地址线</h3><p>20位地址线只能访问1MB的内存，打开A20地址线可以访问4MB的内存。这是为了在80286（可以访问4MB），下兼容8086（访问1MB）。A20默认是关闭的，打开后，就可以访问4MB的内存。开关是通过8042键盘控制器来控制的，</p><p>可以通过操作操作端口92h来实现。</p><h3 id="设置cr0寄存器">4. 设置cr0寄存器</h3><p>cro寄存器的第0位是PE位，此位0表示实模式，1表示保护模式。设置为1，进入保护模式.</p><p>在设置完后cpu就进入保护模式了，但是cs的值仍然是实模式下的值，需要将代码段的选择子装入cs。</p><h3 id="设置cs">5. 设置cs</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asm">jmp dword selector_code:0<br></code></pre></td></tr></table></figure><p>会把 SelectorCode32 装入 cs,并跳转到 Code32Selector:0 处</p><h2 id="从保护模式到实模式">从保护模式到实模式</h2><h3 id="将normal-selector装入dsesfsgsss">1. 将normalselector装入ds，es，fs，gs，ss</h3><p>以使对应段描述符高速缓冲寄存器中含有合适的段界限和属性 ### 2.设置cr0寄存器为0</p><h3 id="跳转回实模式">3. 跳转回实模式</h3><p>jmp cs_real_mode:LABEL_REAL_ENTRY</p><h3id="重新设置段寄存器的值关闭a20地址线并打开中断">4.重新设置段寄存器的值，关闭A20地址线并打开中断</h3>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人脸补全对人脸识别有用吗</title>
    <link href="/2024/09/24/%E4%BA%BA%E8%84%B8%E8%A1%A5%E5%85%A8%E5%AF%B9%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%9C%89%E7%94%A8%E5%90%97/"/>
    <url>/2024/09/24/%E4%BA%BA%E8%84%B8%E8%A1%A5%E5%85%A8%E5%AF%B9%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%9C%89%E7%94%A8%E5%90%97/</url>
    
    <content type="html"><![CDATA[<p>[1]Does Generative Face Completion Help Face Recognition?https://doi.org/10.48550/arXiv.1906.02858<br />总的来说遮挡人脸识别技术可以分为提高鲁棒性和对遮挡人脸进行补全。在该篇论文中采用“玩遮挡游戏”的方法，评估了补全对遮挡人脸识别的影响。<br />事实上， 1. 遮挡人脸补全只有助于提高轻度遮挡如眼镜。 2.而对于手，麦克风,帽子等，在低FAR下只有小幅改进。<br />注： 该测试是在改论文自己实现的补全网络下完成的。</p><p>感觉论文的结论没有什么意义。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人脸识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pathlib 中的 Path类</title>
    <link href="/2024/09/23/pathlib-%E4%B8%AD%E7%9A%84-Path%E7%B1%BB/"/>
    <url>/2024/09/23/pathlib-%E4%B8%AD%E7%9A%84-Path%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="path-创建">Path 创建</h2><p>Path对象可以由一个path_str创建而来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><span class="hljs-built_in">dir</span> = Path(<span class="hljs-string">&quot;lfw&quot;</span>)<br><br></code></pre></td></tr></table></figure><h2 id="path-的拼接">Path 的拼接</h2><p>不同于os.join来对子文件夹进行凭借，Path对象可以方便的使用<code>/</code>符号将Path和str之间拼接起来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">file = <span class="hljs-string">&quot;a.img&quot;</span><br>file_path = <span class="hljs-built_in">dir</span> / file<br></code></pre></td></tr></table></figure><h2 id="path-文件的walk">Path 文件的walk</h2><p>不同于os.walk对文件递归遍历，Path对象使用path.glob("<code>正则表达表达式</code>")对子文件夹搜索。</p><ol type="1"><li>path.glob(dir)对dir下的文件和目录进行搜索，<strong>不递归</strong></li><li>path.rglob(dir)对dir下的文件和目录搜索，<strong>递归搜索</strong></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">cwd.iterdir()<br><span class="hljs-comment"># 是一个迭代器</span><br>path_generate = path.rglob(<span class="hljs-string">&quot;*.jpg&quot;</span>)<br><span class="hljs-comment"># path_generate是一个generate对象，只能用next（）,或者for循环提取</span><br>path_list = <span class="hljs-built_in">list</span>(path_generate)<br><span class="hljs-comment"># path_list 是一个list</span><br></code></pre></td></tr></table></figure><h2 id="path-对象的部分访问">Path 对象的部分访问</h2><h3 id="上级目录">上级目录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">image_file.parent<br><span class="hljs-comment"># Path(&#x27;/home/bexgboost/downloads&#x27;)</span><br>image_file.parents<br><span class="hljs-string">&#x27;&#x27;&#x27; </span><br><span class="hljs-string">[PosixPath(&#x27;/home/bexgboost/downloads&#x27;),</span><br><span class="hljs-string">PosixPath(&#x27;/home/bexgboost&#x27;),</span><br><span class="hljs-string">PosixPath(&#x27;/home&#x27;),</span><br><span class="hljs-string">PosixPath(&#x27;/&#x27;)],list</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br></code></pre></td></tr></table></figure><h3 id="文件名后缀-无后缀">文件名，后缀, 无后缀</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">image_file.name<br><span class="hljs-comment">#&#x27;midjourney.png&#x27;</span><br>image_file.suffix<br><span class="hljs-comment">#&#x27;.png&#x27;</span><br>image_file.stem<br><span class="hljs-comment">#&#x27;midjourney&#x27;</span><br><br></code></pre></td></tr></table></figure><h2 id="path对象是否存在">Path对象是否存在</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">entry.is_dir()<br>entry.is_file()<br>entry.exists()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pathlib</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CNN</title>
    <link href="/2024/09/23/CNN/"/>
    <url>/2024/09/23/CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="convolutional-neural-networks">Convolutional NeuralNetworks</h1><p>一个非常好的cnn计算过程演示网站--https://poloclub.github.io/cnn-explainer/#article-flatten</p><img src="/2024/09/23/CNN/image.png" class="" title="image"><p>之前介绍的全连接的神经网络中使用了全连接层（Affine层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。全连接层存在什么问题呢？那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。实际上，前面提到的使用了MNIST数据集的例子中，输入图像就是1通道、高28像素、长28像素 的（1, 28,28）形状，但却被排成1列，以784个数据的形式输入到最开始的 Affine层。图像是3维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，可以（有可能）正确理解图像等具有形状的数据。</p><h2 id="filter">filter</h2><ul><li>filter is a matrix of weights that are learned during the trainingprocess. ## padding</li><li>padding is a technique used to preserve the spatial dimensions ofthe input volume. ## stride</li><li>stride is the number of pixels by which we slide the filter matrixover the input volume. ## pooling<br />池化是缩小高、长方向上的空间的运算。池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。<br />输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对<br />输入数据的微小偏差具有鲁棒性。 <img src="/2024/09/23/CNN/image-1.png" class="" title="image-1"></li></ul><h1 id="resnet">ResNet</h1><p><ahref="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">DeepResidual Learning for Image Recognition</a>论文精读</p><h2 id="深度网络的学习方式">深度网络的学习方式</h2><p>深度神经网络学习的特征可以被分为浅层/中层/深层，by</p><p>M. D. Zeiler and R. Fergus. Visualizing and understandingconvolutional neural networks. In ECCV, 2014.</p><ul><li>浅层学习的是图片中的边缘特征，</li><li>中层学习的是边缘组合形成的图形</li><li>深层学习的是对中层中图形的组合形成的图像</li></ul><p>然后再通过end-to-end的方式进行分类。</p><p>所以特征的层次可以通过堆叠层数来加强。即层数越高，浅层，中层，深层学习到的东西越多。</p><p>于是产生了一个想法是层数越多，模型的学习能力越强，我们只需要设计更深的网络模型即可。</p><p>但是受到了一个障碍的限制即<strong>梯度消失/爆炸（ vanishing/explodinggradients ）</strong></p><p>这个问题可以在一定程度上通过规则化的参数初始化和中间初始化层来解决，这使得几十层的网络的训练成为可能。</p><p>但是随着网络层数的不断提高，网络出现了退化的情况。而且这种退化不是由于过拟合引起的。</p><img src="/2024/09/23/CNN/image-20240812155603287.png" class="" title="image-20240812155603287"><h2id="考虑一个浅层网络和一个深层网络">考虑一个浅层网络和一个深层网络</h2><p>这个深层网络的前部与浅层网络有完全相同的结构。</p><p><strong>思考为什么深层网络的性能会比浅层网络更差</strong></p><p>有一种<strong>假设</strong>起码可以使深层网络具有和浅层网络具有一样的性能。</p><blockquote><p>即这个深层网络的前部具有与浅层网络相同的参数，而后部可以作为一个恒等函数，这样深层网络就可以等价为与浅层网络相同的函数。</p></blockquote><p>这种<strong>假设</strong>证明了深层网络不应该比浅层网络更差。<strong>但现有的手段无法实现上述假设</strong></p><p>于是在该论文中提出了残差块这一结构来实现上述论证。</p><h2 id="残差块">残差块</h2><img src="/2024/09/23/CNN/image-20240812161255587.png" class="" title="image-20240812161255587"><p>何为残差，即输出与输入的差</p><p>我们不让网络去直接拟合预期的输出，而是利用他去拟合残差</p><p>我们将期望的输出定义为 <spanclass="math display">\[H(x)\]</span>，将网络拟合的函数<spanclass="math display">\[F(x)\]</span>（残差函数）定义为：</p><p><span class="math display">\[F(x) = H(x) - x\]</span></p><p>同时有期望输出H(x)为 <span class="math display">\[H(x) = F(x) + x\]</span> 我们假定去拟合<spanclass="math display">\[F(x)\]</span>比拟合<spanclass="math display">\[H(x)\]</span>更加的容易</p><p>让我们用残差块来考虑上一节的假设，用残差块拟合成恒等函数要比一堆非线性成恒等函数要简单，也即将<spanclass="math display">\[F(x)\]</span>拟合为0比将<spanclass="math display">\[H(x)\]</span>拟合为1要容易，这也是由于初始化参数时的正负分布的特性。</p><p>这个网络是通过具有shortcut连接的前馈层来实现的，shortcut连接是指跳过一层或者多层的连接。于是shortcut连接就可以作为恒等映射（函数）。shortcut连接既不增加额外参数也不增加计算复杂度。</p><p>通过残差块我们能够构建152层的网络，并能训练成功（具有相同层数但是没有残差块的网络会在训练过程中退化），而且残差网络具能够通过提高网络的层次来提高正确率。</p><h2 id="深度残差学习">深度残差学习</h2><h3 id="残差块实现">残差块实现</h3><p><span class="math inline">\(F(x)\)</span>和<spanclass="math inline">\(H(x)\)</span>相比的优点是更容易学习。这是由于网络在向恒等学习时更加困难（网络退化现象假设表明），通过残差函数如果恒等是最优的，网络可能很容易的将权值向0移动，然后形成恒等表示，即将<spanclass="math display">\[F(x)\]</span>x向0学习，从而<strong>间接</strong>将<spanclass="math display">\[H(x)\]</span>学习为恒等表示。实验表明，残差函数具有很小的扰动，因此恒等映射是更好的预处理。</p><p>残差块被定义为: <span class="math display">\[y = F(x,{W_i}) + x\]</span> x,y是输入和输出，<spanclass="math display">\[F(x,W_i)\]</span>是需要学习的残差映射。</p><p>以图二为例，详细表示为： <span class="math display">\[y=W_2\sigma(W_1x + b_1)+b_2 +x\]</span> 其中bias在图中被省略了。</p><p><spanclass="math inline">\(F(x)\)</span>的输出维数必须要和x相同，如何不是我们可以在x前乘上<spanclass="math display">\[W_s\]</span>(只在匹配维度时使用)</p><p>残差函数不一定要与图二相同，可以是其他结构或层数，例如卷积层。</p><h3 id="网络架构">网络架构</h3><img src="/2024/09/23/CNN/image-20240812173827614.png" class="" title="image-20240812173827614"><blockquote><p>左:参考VGG-19模型(196亿FLOPs)。</p><p>中:普通网络，34个参数层(36亿FLOPs)。</p><p>右:34个参数层(36亿FLOPs)的残余网络。虚线快捷方式增加了维度。表1显示了更多细节和其他变体。</p></blockquote><p>实现和虚线是shortcut连接，其中虚线代表连接的两层维数不同，采用0填充方式扩展维度。</p><p>这个网络可以说是非常的长。</p><h4 id="普通网络">普通网络</h4><img src="/2024/09/23/CNN/image-20240812174402346.png" class="" title="image-20240812174402346"><p>普通网络在从18层变为34层后具有更高的验证集误差，出现了退化现象。在整个训练过程中34层网络的正确率都低于18层网络</p><p>由于BN层的应用，作者推断优化消失不是由于梯度消失引起的。34层的网络依旧可以达到可观的精度（虽然比不上18层网络），作责推测是由于收敛速率降低导致的，具体原因会在后续研究（有人能告诉是哪篇文章吗）。</p><h4 id="残差网络">残差网络</h4><img src="/2024/09/23/CNN/image-20240812182648469.png" class="" title="image-20240812182648469"><p>在运用了残差网络后，网络退化现象消失了，且34层网络具有更高的精度。</p><p>且在18层时与普通网络相比，收敛速度更快。</p><p>实验还研究了50/101/152层的残差网络，精度都得到了显著的提升。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CNN</tag>
      
      <tag>深度学习基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
